{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c90bc476-c6c7-4683-8a2e-3447d7c86a81",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# ➕ Extensión: Optimización de Consultas y Comparación\n",
    "\n",
    "## 🎯 Objetivo\n",
    "\n",
    "Complementar el laboratorio anterior evaluando el impacto de la caché, el broadcast join y Adaptive Query Execution (AQE) en una tarea práctica de análisis y agregación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a29eae6-a2f9-411b-829e-379cc665c6ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 🧠 Técnicas de optimización incluidas\n",
    "- Comparación de tiempo de ejecución con y sin caché\n",
    "- Uso de `persist(StorageLevel.DISK_ONLY)` para pruebas controladas\n",
    "- Control de broadcast join y AQE dinámicamente\n",
    "- Análisis del plan de ejecución\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd846cab-0e67-4c14-a5c0-904cf6f76ffe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Importar StorageLevel para control de persistencia\n",
    "from pyspark import StorageLevel\n",
    "from pyspark.sql.functions import col\n",
    "from time import time\n",
    "\n",
    "# Crear dataset base\n",
    "df_base = spark.range(0, 50_000_000).withColumn(\"mod\", (col(\"id\") % 1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e092fe54-2720-4419-aaf1-6d8f92205209",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 🔁 Experimento A: Sin caché ni optimización\n",
    "Medimos el tiempo de un `groupBy().count()` sin ninguna persistencia ni AQE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d0171209-7e82-4f90-b888-8a590f493bc5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "start = time()\n",
    "df_base.groupBy(\"mod\").count().collect()\n",
    "print(f\"Tiempo sin caché ni AQE: {round(time() - start, 2)} segundos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4494dbdc-8825-4503-833e-6d8c969c66a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ⚙️ Activar AQE y caché"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "106b5850-13b0-4a4c-9749-f6541ecfbcfe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.adaptive.enabled\", True)\n",
    "df_base.cache().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2aef7463-b672-45f0-a48d-2f8102bb18ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 🔁 Experimento B: Con caché y AQE activado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "865e2d5b-4220-43f3-b1fb-7b879113afa6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "start = time()\n",
    "df_base.groupBy(\"mod\").count().collect()\n",
    "print(f\"Tiempo con caché y AQE: {round(time() - start, 2)} segundos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ce893598-2dd8-4faf-89e9-bed600d5879b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "En escenarios de datasets moderados y clusters sin alta carga, técnicas como cache() o AQE pueden no mostrar mejoras inmediatas y, de hecho, introducir ligero overhead por su inicialización.\n",
    "Sin embargo, su verdadero valor se evidencia cuando el dataset es consultado múltiples veces, o en pipelines complejos donde evitar recomputaciones es crítico."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "894a9ad6-4623-4003-9baf-c762ec2ae06d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Objetivo\n",
    "Demostrar cómo el uso de `cache()` y `Adaptive Query Execution` mejora el rendimiento cuando un DataFrame se reutiliza múltiples veces en operaciones pesadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38503901-7555-4efa-a9d1-487863728773",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Generar un DataFrame con datos sintéticos de 100 millones de registros\n",
    "from pyspark.sql.functions import rand\n",
    "df = spark.range(0, 100_000_000).withColumn(\"value\", rand())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "de37e058-69f5-4efd-a658-97c6a5a2f427",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Caso A: Sin caché, sin AQE (tres agregaciones secuenciales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b3ed0fc-bd04-4864-b94d-a3989e5df888",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.adaptive.enabled\", False)\n",
    "import time\n",
    "start = time.time()\n",
    "df.filter(\"value > 0.9\").count()\n",
    "df.groupBy((df.id % 100)).count().collect()\n",
    "df.filter(\"value < 0.1\").agg({\"value\": \"avg\"}).collect()\n",
    "print(f\" Tiempo total sin caché ni AQE: {round(time.time() - start, 2)} seg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cd31b25a-e452-43f2-b0b3-24377716a17d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Caso B: Con caché y AQE, para evitar recomputaciones y optimizar joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4c2a83d-b2b5-4bfa-8abf-1ea758dba545",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.cache().count()  # Forzar persistencia en memoria\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", True)\n",
    "start = time.time()\n",
    "df.filter(\"value > 0.9\").count()\n",
    "df.groupBy((df.id % 100)).count().collect()\n",
    "df.filter(\"value < 0.1\").agg({\"value\": \"avg\"}).collect()\n",
    "print(f\" Tiempo total con caché y AQE: {round(time.time() - start, 2)} seg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "96c1377e-4c98-41bd-b774-925c812a4b72",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Con `cache()` y AQE activado, las operaciones que reutilizan el DataFrame se benefician al no recalcular el plan lógico desde cero ni volver a leer datos de disco.\n",
    "Este ejemplo simula un patrón real de exploración de datos masiva en pipelines de análisis iterativo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aaa3fefe-fa8f-45c0-a6ee-60effe33256f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ✅ Comparación final\n",
    "- Revisa Spark UI para ver diferencias en el plan físico.\n",
    "- Compara tiempo de ejecución y stages utilizados.\n",
    "- Observa si hubo broadcast joins no deseados.\n",
    "\n",
    "Este mini proyecto demuestra cómo pequeñas configuraciones pueden mejorar significativamente el rendimiento sin necesidad de escalar el cluster."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Lab10_Databricks_Cluster_Avanzado_Parte2",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
