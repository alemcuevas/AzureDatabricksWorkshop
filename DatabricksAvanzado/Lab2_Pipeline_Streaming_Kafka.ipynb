{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# \ud83e\uddea Laboratorio 2: Implementaci\u00f3n de un Pipeline de Streaming con Kafka\n",
        "\n",
        "## \ud83c\udfaf Objetivo\n",
        "\n",
        "Implementar un pipeline de ingesta en tiempo real en Azure Databricks usando Apache Kafka y Structured Streaming para procesar flujos de datos en tiempo real."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \u2139\ufe0f Introducci\u00f3n\n",
        "\n",
        "La ingesta de datos en tiempo real es esencial para casos de uso como monitoreo de sensores, an\u00e1lisis de logs o recomendaciones en vivo.\n",
        "Este laboratorio muestra c\u00f3mo integrar Apache Kafka o Event Hubs con Databricks para habilitar an\u00e1lisis en tiempo real usando Structured Streaming."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \ud83e\udde0 Conceptos Clave\n",
        "\n",
        "**Apache Kafka**\n",
        "Sistema distribuido de mensajer\u00eda altamente escalable usado para procesar flujos de eventos en tiempo real.\n",
        "[\ud83d\udcd8 Ver documentaci\u00f3n](https://kafka.apache.org/)\n",
        "\n",
        "**Azure Event Hubs**\n",
        "Servicio de Azure equivalente a Kafka, compatible con el protocolo Kafka y utilizado para ingesta de datos en tiempo real.\n",
        "[\ud83d\udcd8 Ver documentaci\u00f3n](https://learn.microsoft.com/en-us/azure/event-hubs/)\n",
        "\n",
        "**Structured Streaming**\n",
        "API de Spark para procesamiento incremental y continuo de flujos de datos. Permite tratar flujos como si fueran DataFrames.\n",
        "[\ud83d\udcd8 Ver documentaci\u00f3n](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \ud83d\udee0\ufe0f Pasos del laboratorio\n",
        "\n",
        "### 1. Configurar acceso a Kafka o Event Hubs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "kafka_bootstrap_servers = \"<broker>:9092\"\n",
        "topic = \"eventos\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. Leer datos desde Kafka usando Structured Streaming"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_kafka = spark.readStream.format(\"kafka\") \\\n",
        "    .option(\"kafka.bootstrap.servers\", kafka_bootstrap_servers) \\\n",
        "    .option(\"subscribe\", topic) \\\n",
        "    .load()\n",
        "\n",
        "df_valores = df_kafka.selectExpr(\"CAST(value AS STRING)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3. Procesar datos en tiempo real (ejemplo de parsing JSON)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import from_json, col\n",
        "from pyspark.sql.types import StructType, StringType, IntegerType\n",
        "\n",
        "schema = StructType() \\\n",
        "    .add(\"id\", IntegerType()) \\\n",
        "    .add(\"evento\", StringType())\n",
        "\n",
        "df_parsed = df_valores.withColumn(\"json\", from_json(col(\"value\"), schema)) \\\n",
        "    .select(\"json.*\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4. Escribir resultados en consola o Delta Lake (modo append)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_parsed.writeStream \\\n",
        "    .format(\"delta\") \\\n",
        "    .outputMode(\"append\") \\\n",
        "    .option(\"checkpointLocation\", \"/tmp/checkpoint/kafka\") \\\n",
        "    .start(\"/mnt/datalake/streaming_output\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \u2705 Validaci\u00f3n\n",
        "- Confirma que los datos llegan desde Kafka y son procesados correctamente.\n",
        "- Verifica que los resultados se almacenen en Delta Lake.\n",
        "- Monitorea la latencia del stream desde el UI de Spark Structured Streaming."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}