{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "965c6ee5",
   "metadata": {},
   "source": [
    "## 1. Verificaci√≥n del Entorno MLflow\n",
    "\n",
    "MLflow viene pre-instalado en Azure Databricks. Verificamos la configuraci√≥n inicial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b5811c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar versi√≥n de MLflow instalada\n",
    "import mlflow\n",
    "print(f\"MLflow version: {mlflow.__version__}\")\n",
    "\n",
    "# Verificar URI de tracking (apunta al workspace de Databricks)\n",
    "print(f\"Tracking URI: {mlflow.get_tracking_uri()}\")\n",
    "\n",
    "# Importar librer√≠as necesarias\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"\\n‚úì Librer√≠as importadas exitosamente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aec2690",
   "metadata": {},
   "source": [
    "## 2. Configuraci√≥n del Experimento\n",
    "\n",
    "Creamos un experimento para organizar nuestros runs de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487936cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear o configurar un experimento\n",
    "# Reemplaza <tu-usuario> con tu nombre de usuario de Databricks\n",
    "experiment_name = \"/Users/<tu-usuario>/energy-prediction-experiment\"\n",
    "\n",
    "# Configurar el experimento\n",
    "mlflow.set_experiment(experiment_name)\n",
    "\n",
    "# Obtener informaci√≥n del experimento\n",
    "experiment = mlflow.get_experiment_by_name(experiment_name)\n",
    "print(f\"Experiment ID: {experiment.experiment_id}\")\n",
    "print(f\"Artifact Location: {experiment.artifact_location}\")\n",
    "print(f\"\\n‚úì Experimento configurado exitosamente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937089da",
   "metadata": {},
   "source": [
    "## 3. Carga y Preparaci√≥n de Datos\n",
    "\n",
    "Cargamos los datos procesados del Lab 3 o datos originales si es necesario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0723086f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opci√≥n 1: Cargar desde Delta Lake (resultado del Lab 3)\n",
    "try:\n",
    "    df_energy = spark.read.format(\"delta\").load(\"/delta/energy_features\")\n",
    "    print(\"‚úì Datos cargados desde Delta Lake\")\n",
    "except:\n",
    "    # Opci√≥n 2: Cargar desde archivo CSV original\n",
    "    df_energy = spark.read.csv(\"/FileStore/tables/owid-energy-data.csv\", header=True, inferSchema=True)\n",
    "    print(\"‚úì Datos cargados desde CSV original\")\n",
    "\n",
    "# Convertir a Pandas para este ejemplo\n",
    "df = df_energy.toPandas()\n",
    "\n",
    "print(f\"\\nDataset shape: {df.shape}\")\n",
    "print(f\"Columnas disponibles: {len(df.columns)}\")\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70fdaa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparaci√≥n de datos para modelado\n",
    "# Objetivo: Predecir el nivel de consumo energ√©tico\n",
    "\n",
    "# Seleccionar features relevantes\n",
    "df_model = df[['year', 'population', 'gdp', \n",
    "               'primary_energy_consumption', \n",
    "               'renewables_consumption',\n",
    "               'fossil_fuel_consumption']].copy()\n",
    "\n",
    "# Remover filas con valores nulos\n",
    "df_model = df_model.dropna()\n",
    "\n",
    "# Crear features adicionales\n",
    "df_model['renewable_ratio'] = df_model['renewables_consumption'] / (df_model['primary_energy_consumption'] + 1)\n",
    "df_model['energy_per_capita'] = (df_model['primary_energy_consumption'] / df_model['population']) * 1000000\n",
    "df_model['fossil_ratio'] = df_model['fossil_fuel_consumption'] / (df_model['primary_energy_consumption'] + 1)\n",
    "\n",
    "print(f\"‚úì Features creados\")\n",
    "print(f\"Dataset limpio: {df_model.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f18814",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear target: clasificar pa√≠ses por consumo per c√°pita\n",
    "df_model['energy_class'] = pd.cut(\n",
    "    df_model['energy_per_capita'], \n",
    "    bins=[0, 30, 70, 150, float('inf')],\n",
    "    labels=['Low', 'Medium', 'High', 'Very High']\n",
    ")\n",
    "\n",
    "# Preparar features (X) y target (y)\n",
    "feature_cols = [\n",
    "    'year', 'population', 'gdp',\n",
    "    'primary_energy_consumption',\n",
    "    'renewables_consumption',\n",
    "    'fossil_fuel_consumption',\n",
    "    'renewable_ratio',\n",
    "    'fossil_ratio'\n",
    "]\n",
    "\n",
    "X = df_model[feature_cols]\n",
    "y = df_model['energy_class']\n",
    "\n",
    "# Codificar target\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)\n",
    "\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"\\nTarget distribution:\")\n",
    "print(pd.Series(y).value_counts())\n",
    "print(f\"\\nClases: {le.classes_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab5a269",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split estratificado\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_encoded, \n",
    "    test_size=0.2, \n",
    "    random_state=42,\n",
    "    stratify=y_encoded\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape}\")\n",
    "print(f\"Test set: {X_test.shape}\")\n",
    "print(f\"\\nTrain class distribution: {np.bincount(y_train)}\")\n",
    "print(f\"Test class distribution: {np.bincount(y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b85395",
   "metadata": {},
   "source": [
    "## 4. Primer Modelo con MLflow Tracking\n",
    "\n",
    "Entrenaremos un modelo Random Forest con logging completo en MLflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188d8112",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iniciar un run de MLflow\n",
    "with mlflow.start_run(run_name=\"random_forest_baseline\") as run:\n",
    "    \n",
    "    # 1. Log de par√°metros\n",
    "    n_estimators = 100\n",
    "    max_depth = 10\n",
    "    random_state = 42\n",
    "    \n",
    "    mlflow.log_param(\"model_type\", \"RandomForest\")\n",
    "    mlflow.log_param(\"n_estimators\", n_estimators)\n",
    "    mlflow.log_param(\"max_depth\", max_depth)\n",
    "    mlflow.log_param(\"random_state\", random_state)\n",
    "    mlflow.log_param(\"test_size\", 0.2)\n",
    "    \n",
    "    # 2. Entrenar modelo\n",
    "    model = RandomForestClassifier(\n",
    "        n_estimators=n_estimators,\n",
    "        max_depth=max_depth,\n",
    "        random_state=random_state\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # 3. Predicciones\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_test = model.predict(X_test)\n",
    "    \n",
    "    # 4. Calcular m√©tricas\n",
    "    train_accuracy = accuracy_score(y_train, y_pred_train)\n",
    "    test_accuracy = accuracy_score(y_test, y_pred_test)\n",
    "    precision = precision_score(y_test, y_pred_test, average='weighted')\n",
    "    recall = recall_score(y_test, y_pred_test, average='weighted')\n",
    "    f1 = f1_score(y_test, y_pred_test, average='weighted')\n",
    "    \n",
    "    # 5. Log de m√©tricas\n",
    "    mlflow.log_metric(\"train_accuracy\", train_accuracy)\n",
    "    mlflow.log_metric(\"test_accuracy\", test_accuracy)\n",
    "    mlflow.log_metric(\"precision\", precision)\n",
    "    mlflow.log_metric(\"recall\", recall)\n",
    "    mlflow.log_metric(\"f1_score\", f1)\n",
    "    \n",
    "    # 6. Log del modelo\n",
    "    mlflow.sklearn.log_model(\n",
    "        model, \n",
    "        \"random_forest_model\",\n",
    "        registered_model_name=\"energy_classifier_rf\"\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úì Run ID: {run.info.run_id}\")\n",
    "    print(f\"‚úì Test Accuracy: {test_accuracy:.4f}\")\n",
    "    print(f\"‚úì F1 Score: {f1:.4f}\")\n",
    "    print(f\"\\n‚ûú Ve al Experiments tab para ver los resultados completos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a67acd7e",
   "metadata": {},
   "source": [
    "## 5. Visualizaci√≥n de Resultados\n",
    "\n",
    "Creamos artefactos visuales que se registrar√°n en MLflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b774639e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear visualizaciones con el √∫ltimo modelo entrenado\n",
    "with mlflow.start_run(run_name=\"rf_with_visualizations\") as run:\n",
    "    \n",
    "    # Entrenar modelo\n",
    "    model = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Log m√©tricas\n",
    "    mlflow.log_metric(\"test_accuracy\", accuracy_score(y_test, y_pred))\n",
    "    mlflow.log_metric(\"f1_score\", f1_score(y_test, y_pred, average='weighted'))\n",
    "    \n",
    "    # 1. Matriz de confusi√≥n\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=le.classes_, yticklabels=le.classes_)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"/tmp/confusion_matrix.png\")\n",
    "    mlflow.log_artifact(\"/tmp/confusion_matrix.png\", \"visualizations\")\n",
    "    plt.show()\n",
    "    \n",
    "    # 2. Feature importance\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': feature_cols,\n",
    "        'importance': model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(feature_importance['feature'], feature_importance['importance'])\n",
    "    plt.xlabel('Importance')\n",
    "    plt.title('Feature Importance')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"/tmp/feature_importance.png\")\n",
    "    mlflow.log_artifact(\"/tmp/feature_importance.png\", \"visualizations\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Log modelo\n",
    "    mlflow.sklearn.log_model(model, \"model\")\n",
    "    \n",
    "    print(\"‚úì Visualizaciones creadas y registradas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acbd6b64",
   "metadata": {},
   "source": [
    "## 6. Comparaci√≥n de M√∫ltiples Modelos\n",
    "\n",
    "Entrenaremos varios algoritmos y los compararemos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad09ebb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_log_model(model, model_name, params, X_train, X_test, y_train, y_test):\n",
    "    \"\"\"\n",
    "    Funci√≥n para entrenar y registrar modelos con MLflow\n",
    "    \"\"\"\n",
    "    with mlflow.start_run(run_name=model_name):\n",
    "        \n",
    "        # Log de par√°metros\n",
    "        mlflow.log_param(\"model_type\", model_name)\n",
    "        for param_name, param_value in params.items():\n",
    "            mlflow.log_param(param_name, param_value)\n",
    "        \n",
    "        # Entrenar modelo\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Predicciones\n",
    "        y_pred_train = model.predict(X_train)\n",
    "        y_pred_test = model.predict(X_test)\n",
    "        \n",
    "        # M√©tricas\n",
    "        train_accuracy = accuracy_score(y_train, y_pred_train)\n",
    "        test_accuracy = accuracy_score(y_test, y_pred_test)\n",
    "        precision = precision_score(y_test, y_pred_test, average='weighted')\n",
    "        recall = recall_score(y_test, y_pred_test, average='weighted')\n",
    "        f1 = f1_score(y_test, y_pred_test, average='weighted')\n",
    "        \n",
    "        # Log de m√©tricas\n",
    "        mlflow.log_metric(\"train_accuracy\", train_accuracy)\n",
    "        mlflow.log_metric(\"test_accuracy\", test_accuracy)\n",
    "        mlflow.log_metric(\"precision\", precision)\n",
    "        mlflow.log_metric(\"recall\", recall)\n",
    "        mlflow.log_metric(\"f1_score\", f1)\n",
    "        mlflow.log_metric(\"overfitting\", train_accuracy - test_accuracy)\n",
    "        \n",
    "        # Log del modelo\n",
    "        mlflow.sklearn.log_model(\n",
    "            model, \n",
    "            f\"{model_name}_model\",\n",
    "            registered_model_name=f\"energy_classifier_{model_name.lower().replace(' ', '_')}\"\n",
    "        )\n",
    "        \n",
    "        print(f\"‚úì {model_name} - Test Accuracy: {test_accuracy:.4f}, F1: {f1:.4f}\")\n",
    "        \n",
    "        return model, test_accuracy, f1\n",
    "\n",
    "print(\"‚úì Funci√≥n de entrenamiento definida\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c51dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diccionario de modelos a probar\n",
    "models_config = {\n",
    "    \"Random Forest\": {\n",
    "        \"model\": RandomForestClassifier(random_state=42),\n",
    "        \"params\": {\n",
    "            \"n_estimators\": 100,\n",
    "            \"max_depth\": 15,\n",
    "            \"min_samples_split\": 5\n",
    "        }\n",
    "    },\n",
    "    \"Gradient Boosting\": {\n",
    "        \"model\": GradientBoostingClassifier(random_state=42),\n",
    "        \"params\": {\n",
    "            \"n_estimators\": 100,\n",
    "            \"learning_rate\": 0.1,\n",
    "            \"max_depth\": 5\n",
    "        }\n",
    "    },\n",
    "    \"Logistic Regression\": {\n",
    "        \"model\": LogisticRegression(max_iter=1000, random_state=42),\n",
    "        \"params\": {\n",
    "            \"max_iter\": 1000,\n",
    "            \"C\": 1.0,\n",
    "            \"solver\": \"lbfgs\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Entrenar todos los modelos\n",
    "results = {}\n",
    "\n",
    "for model_name, config in models_config.items():\n",
    "    model = config[\"model\"]\n",
    "    params = config[\"params\"]\n",
    "    \n",
    "    # Configurar par√°metros del modelo\n",
    "    model.set_params(**params)\n",
    "    \n",
    "    # Entrenar y registrar\n",
    "    trained_model, accuracy, f1 = train_and_log_model(\n",
    "        model, model_name, params, \n",
    "        X_train, X_test, y_train, y_test\n",
    "    )\n",
    "    \n",
    "    results[model_name] = {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"f1\": f1,\n",
    "        \"model\": trained_model\n",
    "    }\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"RESUMEN DE RESULTADOS\")\n",
    "print(\"=\"*50)\n",
    "results_df = pd.DataFrame(results).T\n",
    "display(results_df[['accuracy', 'f1']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2224919f",
   "metadata": {},
   "source": [
    "## 7. B√∫squeda de Hiperpar√°metros\n",
    "\n",
    "Realizaremos Grid Search con logging autom√°tico de todos los resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8f4bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Definir grid de par√°metros\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [5, 10, 15],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "# Iniciar parent run\n",
    "with mlflow.start_run(run_name=\"rf_grid_search\") as parent_run:\n",
    "    \n",
    "    mlflow.log_param(\"search_type\", \"GridSearch\")\n",
    "    mlflow.log_param(\"param_grid\", str(param_grid))\n",
    "    \n",
    "    # Grid Search\n",
    "    rf = RandomForestClassifier(random_state=42)\n",
    "    grid_search = GridSearchCV(\n",
    "        rf, param_grid, \n",
    "        cv=3, \n",
    "        scoring='f1_weighted',\n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    print(\"Ejecutando Grid Search...\")\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Log mejores par√°metros\n",
    "    mlflow.log_params(grid_search.best_params_)\n",
    "    \n",
    "    # Log mejor score\n",
    "    mlflow.log_metric(\"best_cv_score\", grid_search.best_score_)\n",
    "    \n",
    "    # Evaluar en test\n",
    "    y_pred = grid_search.predict(X_test)\n",
    "    test_f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    test_accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    mlflow.log_metric(\"test_f1\", test_f1)\n",
    "    mlflow.log_metric(\"test_accuracy\", test_accuracy)\n",
    "    \n",
    "    # Log del mejor modelo\n",
    "    mlflow.sklearn.log_model(\n",
    "        grid_search.best_estimator_,\n",
    "        \"best_rf_model\",\n",
    "        registered_model_name=\"energy_classifier_rf_optimized\"\n",
    "    )\n",
    "    \n",
    "    # Log de resultados de grid search\n",
    "    cv_results = pd.DataFrame(grid_search.cv_results_)\n",
    "    cv_results.to_csv(\"/tmp/grid_search_results.csv\", index=False)\n",
    "    mlflow.log_artifact(\"/tmp/grid_search_results.csv\")\n",
    "    \n",
    "    print(f\"\\n‚úì Grid Search completado\")\n",
    "    print(f\"Mejores par√°metros: {grid_search.best_params_}\")\n",
    "    print(f\"Mejor CV Score: {grid_search.best_score_:.4f}\")\n",
    "    print(f\"Test F1 Score: {test_f1:.4f}\")\n",
    "    print(f\"Test Accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22996923",
   "metadata": {},
   "source": [
    "## 8. MLflow Model Registry\n",
    "\n",
    "Gestionaremos el ciclo de vida de los modelos usando Model Registry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917f217c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "# Crear cliente de MLflow\n",
    "client = MlflowClient()\n",
    "\n",
    "# Listar todos los modelos registrados\n",
    "print(\"Modelos Registrados en Model Registry:\")\n",
    "print(\"=\"*50)\n",
    "registered_models = client.search_registered_models()\n",
    "for rm in registered_models:\n",
    "    print(f\"  üì¶ {rm.name}\")\n",
    "    latest_versions = client.get_latest_versions(rm.name)\n",
    "    for version in latest_versions:\n",
    "        print(f\"     ‚îî‚îÄ Version {version.version}: {version.current_stage}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250d12a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Promover modelo a diferentes stages\n",
    "model_name = \"energy_classifier_rf_optimized\"\n",
    "\n",
    "try:\n",
    "    # Obtener √∫ltima versi√≥n\n",
    "    latest_versions = client.get_latest_versions(model_name)\n",
    "    \n",
    "    if latest_versions:\n",
    "        latest_version = latest_versions[0].version\n",
    "        \n",
    "        # Transici√≥n a \"Staging\"\n",
    "        client.transition_model_version_stage(\n",
    "            name=model_name,\n",
    "            version=latest_version,\n",
    "            stage=\"Staging\",\n",
    "            archive_existing_versions=True\n",
    "        )\n",
    "        \n",
    "        print(f\"‚úì Model {model_name} v{latest_version} promovido a Staging\")\n",
    "        \n",
    "        # Agregar descripci√≥n y tags\n",
    "        client.update_model_version(\n",
    "            name=model_name,\n",
    "            version=latest_version,\n",
    "            description=\"Random Forest optimizado con GridSearch para clasificaci√≥n de consumo energ√©tico\"\n",
    "        )\n",
    "        \n",
    "        client.set_model_version_tag(\n",
    "            name=model_name,\n",
    "            version=latest_version,\n",
    "            key=\"validation_status\",\n",
    "            value=\"passed\"\n",
    "        )\n",
    "        \n",
    "        client.set_model_version_tag(\n",
    "            name=model_name,\n",
    "            version=latest_version,\n",
    "            key=\"dataset\",\n",
    "            value=\"owid_energy\"\n",
    "        )\n",
    "        \n",
    "        print(\"‚úì Metadata actualizada\")\n",
    "    else:\n",
    "        print(f\"‚ö† No se encontraron versiones del modelo {model_name}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ö† Error al promover modelo: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e62f2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar modelo desde Model Registry\n",
    "model_uri = f\"models:/{model_name}/Staging\"\n",
    "\n",
    "try:\n",
    "    loaded_model = mlflow.sklearn.load_model(model_uri)\n",
    "    \n",
    "    # Hacer predicciones\n",
    "    sample_data = X_test.iloc[:5]\n",
    "    predictions = loaded_model.predict(sample_data)\n",
    "    predictions_labels = le.inverse_transform(predictions)\n",
    "    \n",
    "    print(\"‚úì Modelo cargado desde Registry\")\n",
    "    print(\"\\nPredicciones con modelo desde Staging:\")\n",
    "    for i, (pred_label, actual) in enumerate(zip(predictions_labels, y_test[:5])):\n",
    "        actual_label = le.inverse_transform([actual])[0]\n",
    "        print(f\"  Sample {i+1}: Predicci√≥n={pred_label}, Real={actual_label}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ö† Error al cargar modelo: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a05cf5",
   "metadata": {},
   "source": [
    "## 9. Autologging con MLflow\n",
    "\n",
    "MLflow puede registrar autom√°ticamente par√°metros, m√©tricas y modelos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9e36bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activar autologging para scikit-learn\n",
    "mlflow.sklearn.autolog()\n",
    "\n",
    "# Entrenar modelo con autologging\n",
    "with mlflow.start_run(run_name=\"autolog_random_forest\"):\n",
    "    model = RandomForestClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=10,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # MLflow autom√°ticamente registra:\n",
    "    # - Par√°metros del modelo\n",
    "    # - M√©tricas de training\n",
    "    # - Modelo serializado\n",
    "    # - Signature del modelo\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    print(f\"‚úì Modelo entrenado con autologging\")\n",
    "    print(f\"‚úì Accuracy: {accuracy:.4f}\")\n",
    "    print(\"\\n‚ûú MLflow registr√≥ autom√°ticamente:\")\n",
    "    print(\"  ‚Ä¢ Par√°metros del modelo\")\n",
    "    print(\"  ‚Ä¢ M√©tricas de entrenamiento\")\n",
    "    print(\"  ‚Ä¢ Modelo serializado\")\n",
    "    print(\"  ‚Ä¢ Signature del modelo\")\n",
    "\n",
    "# Desactivar autologging\n",
    "mlflow.sklearn.autolog(disable=True)\n",
    "print(\"\\n‚úì Autologging desactivado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e3cf79",
   "metadata": {},
   "source": [
    "## 10. B√∫squeda y Comparaci√≥n de Experimentos\n",
    "\n",
    "Buscamos y comparamos los mejores runs del experimento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af595959",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Buscar runs por m√©tricas\n",
    "runs = mlflow.search_runs(\n",
    "    experiment_ids=[experiment.experiment_id],\n",
    "    filter_string=\"metrics.test_accuracy > 0.5\",\n",
    "    order_by=[\"metrics.test_accuracy DESC\"],\n",
    "    max_results=10\n",
    ")\n",
    "\n",
    "print(\"Top 10 runs por accuracy:\")\n",
    "print(\"=\"*80)\n",
    "if len(runs) > 0:\n",
    "    display(runs[['run_id', 'params.model_type', 'metrics.test_accuracy', \n",
    "                  'metrics.f1_score', 'start_time']].head(10))\n",
    "else:\n",
    "    print(\"No se encontraron runs que cumplan el criterio\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbed457f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparaci√≥n visual de experimentos\n",
    "runs_comparison = mlflow.search_runs(\n",
    "    experiment_ids=[experiment.experiment_id],\n",
    "    filter_string=\"params.model_type != ''\",\n",
    "    order_by=[\"start_time DESC\"],\n",
    "    max_results=20\n",
    ")\n",
    "\n",
    "if len(runs_comparison) > 0:\n",
    "    # Visualizar comparaci√≥n\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Accuracy comparison\n",
    "    model_accuracy = runs_comparison.groupby('params.model_type')['metrics.test_accuracy'].mean().sort_values(ascending=False)\n",
    "    model_accuracy.plot(kind='bar', ax=axes[0], color='steelblue')\n",
    "    axes[0].set_title('Test Accuracy por Tipo de Modelo (Promedio)')\n",
    "    axes[0].set_xlabel('Tipo de Modelo')\n",
    "    axes[0].set_ylabel('Accuracy')\n",
    "    axes[0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # F1 Score comparison\n",
    "    model_f1 = runs_comparison.groupby('params.model_type')['metrics.f1_score'].mean().sort_values(ascending=False)\n",
    "    model_f1.plot(kind='bar', ax=axes[1], color='coral')\n",
    "    axes[1].set_title('F1 Score por Tipo de Modelo (Promedio)')\n",
    "    axes[1].set_xlabel('Tipo de Modelo')\n",
    "    axes[1].set_ylabel('F1 Score')\n",
    "    axes[1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"‚úì Visualizaci√≥n creada\")\n",
    "else:\n",
    "    print(\"No hay suficientes runs para comparar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c3c1e8",
   "metadata": {},
   "source": [
    "## 11. Funci√≥n de Predicci√≥n como Servicio\n",
    "\n",
    "Creamos una funci√≥n reutilizable para hacer predicciones con modelos del Registry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae73ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_energy_class(model_name, model_stage, input_data):\n",
    "    \"\"\"\n",
    "    Funci√≥n de predicci√≥n que carga modelo desde Registry\n",
    "    \n",
    "    Args:\n",
    "        model_name: Nombre del modelo en Registry\n",
    "        model_stage: Stage del modelo (Production, Staging, etc.)\n",
    "        input_data: DataFrame con features de entrada\n",
    "    \n",
    "    Returns:\n",
    "        Array con predicciones y labels\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Cargar modelo\n",
    "        model_uri = f\"models:/{model_name}/{model_stage}\"\n",
    "        model = mlflow.sklearn.load_model(model_uri)\n",
    "        \n",
    "        # Predecir\n",
    "        predictions = model.predict(input_data)\n",
    "        predictions_labels = le.inverse_transform(predictions)\n",
    "        \n",
    "        return predictions, predictions_labels\n",
    "    except Exception as e:\n",
    "        print(f\"Error al hacer predicci√≥n: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# Ejemplo de uso\n",
    "sample_input = X_test.iloc[:10]\n",
    "\n",
    "predictions, labels = predict_energy_class(\n",
    "    \"energy_classifier_rf_optimized\",\n",
    "    \"Staging\",\n",
    "    sample_input\n",
    ")\n",
    "\n",
    "if predictions is not None:\n",
    "    print(\"‚úì Predicciones desde funci√≥n de servicio:\")\n",
    "    print(\"=\"*50)\n",
    "    for i, label in enumerate(labels):\n",
    "        print(f\"  Sample {i+1}: {label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13800d4",
   "metadata": {},
   "source": [
    "## 12. Registro Completo con Mejores Pr√°cticas\n",
    "\n",
    "Implementamos un entrenamiento con todas las mejores pr√°cticas de reproducibilidad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7580103",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import sys\n",
    "import sklearn\n",
    "\n",
    "def train_reproducible_model(model, model_name, X_train, X_test, y_train, y_test):\n",
    "    \"\"\"\n",
    "    Funci√≥n con todas las mejores pr√°cticas de reproducibilidad\n",
    "    \"\"\"\n",
    "    with mlflow.start_run(run_name=model_name) as run:\n",
    "        \n",
    "        # 1. Tags descriptivos\n",
    "        mlflow.set_tag(\"model_family\", \"tree_based\")\n",
    "        mlflow.set_tag(\"problem_type\", \"classification\")\n",
    "        mlflow.set_tag(\"dataset\", \"owid_energy\")\n",
    "        mlflow.set_tag(\"developer\", \"data_science_team\")\n",
    "        mlflow.set_tag(\"version\", \"1.0.0\")\n",
    "        \n",
    "        # 2. Metadata del dataset\n",
    "        mlflow.log_param(\"train_samples\", len(X_train))\n",
    "        mlflow.log_param(\"test_samples\", len(X_test))\n",
    "        mlflow.log_param(\"n_features\", X_train.shape[1])\n",
    "        mlflow.log_param(\"n_classes\", len(np.unique(y_train)))\n",
    "        mlflow.log_param(\"random_state\", 42)\n",
    "        \n",
    "        # 3. Informaci√≥n del ambiente\n",
    "        mlflow.log_param(\"python_version\", sys.version.split()[0])\n",
    "        mlflow.log_param(\"sklearn_version\", sklearn.__version__)\n",
    "        mlflow.log_param(\"mlflow_version\", mlflow.__version__)\n",
    "        \n",
    "        # 4. Par√°metros del modelo\n",
    "        model_params = model.get_params()\n",
    "        for param, value in model_params.items():\n",
    "            mlflow.log_param(f\"model_{param}\", value)\n",
    "        \n",
    "        # 5. Entrenar con timer\n",
    "        start_time = time.time()\n",
    "        model.fit(X_train, y_train)\n",
    "        training_time = time.time() - start_time\n",
    "        \n",
    "        mlflow.log_metric(\"training_time_seconds\", training_time)\n",
    "        \n",
    "        # 6. M√©tricas completas\n",
    "        y_pred_train = model.predict(X_train)\n",
    "        y_pred_test = model.predict(X_test)\n",
    "        \n",
    "        metrics = {\n",
    "            \"train_accuracy\": accuracy_score(y_train, y_pred_train),\n",
    "            \"test_accuracy\": accuracy_score(y_test, y_pred_test),\n",
    "            \"precision\": precision_score(y_test, y_pred_test, average='weighted'),\n",
    "            \"recall\": recall_score(y_test, y_pred_test, average='weighted'),\n",
    "            \"f1_score\": f1_score(y_test, y_pred_test, average='weighted')\n",
    "        }\n",
    "        \n",
    "        for metric_name, metric_value in metrics.items():\n",
    "            mlflow.log_metric(metric_name, metric_value)\n",
    "        \n",
    "        # 7. Signature del modelo\n",
    "        from mlflow.models.signature import infer_signature\n",
    "        signature = infer_signature(X_train, y_pred_train)\n",
    "        \n",
    "        # 8. Input example\n",
    "        input_example = X_train[:5]\n",
    "        \n",
    "        # 9. Log modelo con toda la metadata\n",
    "        mlflow.sklearn.log_model(\n",
    "            model,\n",
    "            \"model\",\n",
    "            signature=signature,\n",
    "            input_example=input_example,\n",
    "            registered_model_name=f\"{model_name}_reproducible\"\n",
    "        )\n",
    "        \n",
    "        # 10. Guardar configuraci√≥n completa\n",
    "        config = {\n",
    "            \"model_config\": {k: str(v) for k, v in model_params.items()},\n",
    "            \"training_config\": {\n",
    "                \"train_size\": len(X_train),\n",
    "                \"test_size\": len(X_test),\n",
    "                \"random_state\": 42\n",
    "            },\n",
    "            \"performance\": metrics,\n",
    "            \"training_time\": training_time\n",
    "        }\n",
    "        \n",
    "        with open(\"/tmp/model_config.json\", \"w\") as f:\n",
    "            json.dump(config, f, indent=2)\n",
    "        mlflow.log_artifact(\"/tmp/model_config.json\")\n",
    "        \n",
    "        print(f\"‚úì Modelo {model_name} entrenado de forma reproducible\")\n",
    "        print(f\"  Run ID: {run.info.run_id}\")\n",
    "        print(f\"  Test Accuracy: {metrics['test_accuracy']:.4f}\")\n",
    "        print(f\"  Training Time: {training_time:.2f}s\")\n",
    "        \n",
    "        return model, run.info.run_id\n",
    "\n",
    "# Ejecutar entrenamiento reproducible\n",
    "model_rf = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)\n",
    "trained_model, run_id = train_reproducible_model(\n",
    "    model_rf, \n",
    "    \"rf_reproducible\",\n",
    "    X_train, X_test, y_train, y_test\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f6316d",
   "metadata": {},
   "source": [
    "## 13. Resumen del Laboratorio\n",
    "\n",
    "Revisamos lo que hemos aprendido y los resultados obtenidos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6934afe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resumen de experimentos\n",
    "print(\"=\"*60)\n",
    "print(\"RESUMEN DEL LABORATORIO\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Buscar todos los runs\n",
    "all_runs = mlflow.search_runs(\n",
    "    experiment_ids=[experiment.experiment_id],\n",
    "    order_by=[\"metrics.test_accuracy DESC\"]\n",
    ")\n",
    "\n",
    "if len(all_runs) > 0:\n",
    "    print(f\"\\n‚úì Total de runs ejecutados: {len(all_runs)}\")\n",
    "    print(f\"‚úì Mejor accuracy: {all_runs['metrics.test_accuracy'].max():.4f}\")\n",
    "    print(f\"‚úì Mejor F1 score: {all_runs['metrics.f1_score'].max():.4f}\")\n",
    "    \n",
    "    best_run = all_runs.iloc[0]\n",
    "    print(f\"\\nüèÜ Mejor modelo:\")\n",
    "    print(f\"  ‚Ä¢ Run ID: {best_run['run_id']}\")\n",
    "    print(f\"  ‚Ä¢ Modelo: {best_run.get('params.model_type', 'N/A')}\")\n",
    "    print(f\"  ‚Ä¢ Accuracy: {best_run['metrics.test_accuracy']:.4f}\")\n",
    "    print(f\"  ‚Ä¢ F1 Score: {best_run['metrics.f1_score']:.4f}\")\n",
    "else:\n",
    "    print(\"No se encontraron runs en el experimento\")\n",
    "\n",
    "# Listar modelos en Registry\n",
    "print(f\"\\nüì¶ Modelos en Registry:\")\n",
    "registered_models = client.search_registered_models()\n",
    "for rm in registered_models:\n",
    "    print(f\"  ‚Ä¢ {rm.name}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"¬°LABORATORIO COMPLETADO CON √âXITO!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c35a955",
   "metadata": {},
   "source": [
    "## Conclusi√≥n\n",
    "\n",
    "¬°Felicitaciones! Has completado el laboratorio de entrenamiento y registro de modelos con MLflow.\n",
    "\n",
    "### Habilidades Adquiridas:\n",
    "\n",
    "‚úÖ Configuraci√≥n de experimentos en MLflow  \n",
    "‚úÖ Entrenamiento de modelos con tracking completo  \n",
    "‚úÖ Uso de MLflow Model Registry  \n",
    "‚úÖ Comparaci√≥n de m√∫ltiples modelos  \n",
    "‚úÖ Gesti√≥n de artefactos y dependencias  \n",
    "‚úÖ Implementaci√≥n de reproducibilidad  \n",
    "‚úÖ B√∫squeda de hiperpar√°metros con logging  \n",
    "‚úÖ Deployment de modelos desde Registry  \n",
    "\n",
    "### Pr√≥ximos Pasos:\n",
    "\n",
    "- Deployment de modelos en producci√≥n\n",
    "- Monitoreo de modelos en tiempo real\n",
    "- Reentrenamiento autom√°tico\n",
    "- MLOps con Azure DevOps\n",
    "- Serving de modelos con REST APIs\n",
    "\n",
    "**¬°Excelente trabajo!**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
