{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7bc5c72d",
   "metadata": {},
   "source": [
    "## 1. Configuración del Entorno\n",
    "\n",
    "Importamos las librerías necesarias para el análisis exploratorio y feature engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba45f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar librerías necesarias\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Configuración para visualizaciones\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"✓ Librerías importadas exitosamente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7c5625",
   "metadata": {},
   "source": [
    "## 2. Carga de Datos\n",
    "\n",
    "Cargamos el dataset de energía mundial desde DBFS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9786910e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar datos con Spark\n",
    "df_spark = spark.read.csv(\n",
    "    \"/FileStore/tables/owid-energy-data.csv\",\n",
    "    header=True,\n",
    "    inferSchema=True\n",
    ")\n",
    "\n",
    "# Mostrar esquema\n",
    "print(\"Esquema del DataFrame:\")\n",
    "df_spark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c36e98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estadísticas básicas\n",
    "display(df_spark.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c94247",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conteo de registros y columnas\n",
    "print(f\"Total de registros: {df_spark.count():,}\")\n",
    "print(f\"Total de columnas: {len(df_spark.columns)}\")\n",
    "\n",
    "# Mostrar primeras filas\n",
    "display(df_spark.limit(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b2e62a",
   "metadata": {},
   "source": [
    "## 3. Conversión a Pandas para EDA Detallado\n",
    "\n",
    "Convertimos a Pandas para realizar análisis exploratorio más detallado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c027d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir a Pandas para análisis exploratorio\n",
    "# Nota: En producción, considerar usar solo una muestra si el dataset es muy grande\n",
    "df_pandas = df_spark.toPandas()\n",
    "\n",
    "# Información general\n",
    "print(\"Información del DataFrame:\")\n",
    "print(df_pandas.info())\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Primeras filas:\")\n",
    "display(df_pandas.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4302ec",
   "metadata": {},
   "source": [
    "## 4. Análisis de Valores Faltantes\n",
    "\n",
    "Identificamos y visualizamos valores faltantes en el dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df50576a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular porcentaje de valores nulos por columna\n",
    "missing_data = pd.DataFrame({\n",
    "    'column': df_pandas.columns,\n",
    "    'missing_count': df_pandas.isnull().sum(),\n",
    "    'missing_percent': (df_pandas.isnull().sum() / len(df_pandas)) * 100\n",
    "})\n",
    "\n",
    "missing_data = missing_data[missing_data['missing_count'] > 0].sort_values(\n",
    "    'missing_percent', \n",
    "    ascending=False\n",
    ")\n",
    "\n",
    "print(f\"Columnas con valores faltantes: {len(missing_data)}\")\n",
    "display(missing_data.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5cb898",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualización de valores faltantes\n",
    "plt.figure(figsize=(12, 6))\n",
    "top_missing = missing_data.head(20)\n",
    "plt.barh(top_missing['column'], top_missing['missing_percent'])\n",
    "plt.xlabel('Porcentaje de Valores Faltantes (%)')\n",
    "plt.title('Top 20 Columnas con Valores Faltantes')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcdcfe99",
   "metadata": {},
   "source": [
    "## 5. Análisis de Variables Numéricas\n",
    "\n",
    "Exploramos las distribuciones de las variables numéricas clave."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca14d44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seleccionar columnas numéricas\n",
    "numeric_cols = df_pandas.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "# Estadísticas descriptivas detalladas\n",
    "print(f\"Variables numéricas: {len(numeric_cols)}\")\n",
    "display(df_pandas[numeric_cols].describe().T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e211754f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribución de variables clave\n",
    "key_features = [\n",
    "    'gdp', \n",
    "    'population', \n",
    "    'primary_energy_consumption',\n",
    "    'renewables_consumption',\n",
    "    'fossil_fuel_consumption',\n",
    "    'greenhouse_gas_emissions'\n",
    "]\n",
    "\n",
    "# Filtrar solo las que existen y tienen datos\n",
    "key_features_available = [col for col in key_features if col in df_pandas.columns and df_pandas[col].notna().sum() > 0]\n",
    "\n",
    "if len(key_features_available) == 0:\n",
    "    print(\"⚠️ No hay features disponibles para visualizar\")\n",
    "else:\n",
    "    print(f\"Visualizando {len(key_features_available)} features: {key_features_available}\")\n",
    "    \n",
    "    # Calcular número de filas y columnas para subplots\n",
    "    n_features = len(key_features_available)\n",
    "    n_cols = 2\n",
    "    n_rows = (n_features + 1) // 2\n",
    "    \n",
    "    # Crear histogramas\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 5 * n_rows))\n",
    "    \n",
    "    # Asegurar que axes sea siempre un array 1D\n",
    "    if n_features == 1:\n",
    "        axes = [axes]\n",
    "    else:\n",
    "        axes = axes.ravel() if n_features > 2 else axes\n",
    "    \n",
    "    for idx, col in enumerate(key_features_available):\n",
    "        if idx < len(axes):\n",
    "            # Eliminar valores nulos y obtener datos\n",
    "            data = df_pandas[col].dropna()\n",
    "            \n",
    "            if len(data) > 0:\n",
    "                axes[idx].hist(data, bins=50, edgecolor='black', alpha=0.7, color='steelblue')\n",
    "                axes[idx].set_title(f'Distribución de {col}', fontsize=12, fontweight='bold')\n",
    "                axes[idx].set_xlabel(col, fontsize=10)\n",
    "                axes[idx].set_ylabel('Frecuencia', fontsize=10)\n",
    "                axes[idx].grid(True, alpha=0.3)\n",
    "                \n",
    "                # Agregar estadísticas\n",
    "                mean_val = data.mean()\n",
    "                median_val = data.median()\n",
    "                axes[idx].axvline(mean_val, color='red', linestyle='--', linewidth=2, label=f'Media: {mean_val:.2e}')\n",
    "                axes[idx].axvline(median_val, color='green', linestyle='--', linewidth=2, label=f'Mediana: {median_val:.2e}')\n",
    "                axes[idx].legend(fontsize=8)\n",
    "    \n",
    "    # Ocultar ejes no utilizados\n",
    "    for idx in range(len(key_features_available), len(axes)):\n",
    "        fig.delaxes(axes[idx])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04bceabd",
   "metadata": {},
   "source": [
    "## 6. Análisis de Correlaciones\n",
    "\n",
    "Calculamos y visualizamos las correlaciones entre variables de energía."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6755d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seleccionar subconjunto de variables para análisis de correlación\n",
    "energy_features = [col for col in df_pandas.columns \n",
    "                   if any(keyword in col.lower() for keyword in \n",
    "                   ['energy', 'consumption', 'electricity', 'renewables', 'fossil'])]\n",
    "\n",
    "# Agregar variables contextuales\n",
    "correlation_cols = ['year', 'gdp', 'population'] + energy_features[:15]\n",
    "correlation_cols = [col for col in correlation_cols if col in df_pandas.columns]\n",
    "\n",
    "print(f\"Analizando correlaciones para {len(correlation_cols)} variables\")\n",
    "\n",
    "# Calcular matriz de correlación\n",
    "correlation_matrix = df_pandas[correlation_cols].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fef50ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar matriz de correlación\n",
    "if len(correlation_matrix.columns) > 0:\n",
    "    # Determinar tamaño de figura basado en número de variables\n",
    "    n_vars = len(correlation_matrix.columns)\n",
    "    fig_size = max(12, min(20, n_vars * 0.8))\n",
    "    \n",
    "    plt.figure(figsize=(fig_size, fig_size * 0.9))\n",
    "    \n",
    "    # Configurar heatmap con manejo de valores nulos\n",
    "    mask = correlation_matrix.isnull()\n",
    "    \n",
    "    sns.heatmap(\n",
    "        correlation_matrix, \n",
    "        annot=n_vars <= 20,  # Solo anotar si hay 20 o menos variables\n",
    "        fmt='.2f', \n",
    "        cmap='coolwarm',\n",
    "        center=0,\n",
    "        square=True,\n",
    "        linewidths=0.5,\n",
    "        cbar_kws={'label': 'Coeficiente de Correlación'},\n",
    "        mask=mask,\n",
    "        vmin=-1,\n",
    "        vmax=1\n",
    "    )\n",
    "    \n",
    "    plt.title('Matriz de Correlación - Variables de Energía', fontsize=16, fontweight='bold', pad=20)\n",
    "    plt.xticks(rotation=45, ha='right', fontsize=10)\n",
    "    plt.yticks(rotation=0, fontsize=10)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"⚠️ No hay suficientes variables para crear la matriz de correlación\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe10bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identificar correlaciones fuertes\n",
    "threshold = 0.7\n",
    "high_corr = []\n",
    "for i in range(len(correlation_matrix.columns)):\n",
    "    for j in range(i+1, len(correlation_matrix.columns)):\n",
    "        if abs(correlation_matrix.iloc[i, j]) > threshold:\n",
    "            high_corr.append({\n",
    "                'Feature 1': correlation_matrix.columns[i],\n",
    "                'Feature 2': correlation_matrix.columns[j],\n",
    "                'Correlation': correlation_matrix.iloc[i, j]\n",
    "            })\n",
    "\n",
    "print(f\"\\nCorrelaciones Fuertes (|r| > {threshold}):\")\n",
    "display(pd.DataFrame(high_corr).sort_values('Correlation', ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05283ff2",
   "metadata": {},
   "source": [
    "## 7. Análisis Temporal y por País\n",
    "\n",
    "Analizamos la evolución temporal del consumo energético en los principales países."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b70ce87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Análisis de evolución temporal\n",
    "if 'year' in df_pandas.columns and 'country' in df_pandas.columns:\n",
    "    # Top 10 países por consumo energético reciente\n",
    "    recent_year = df_pandas['year'].max()\n",
    "    print(f\"Año más reciente en el dataset: {recent_year}\")\n",
    "    \n",
    "    top_countries = df_pandas[\n",
    "        df_pandas['year'] == recent_year\n",
    "    ].nlargest(10, 'primary_energy_consumption')['country'].tolist()\n",
    "    \n",
    "    print(f\"Top 10 países por consumo energético en {recent_year}:\")\n",
    "    for idx, country in enumerate(top_countries, 1):\n",
    "        print(f\"{idx}. {country}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae79d885",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evolución temporal de consumo energético\n",
    "plt.figure(figsize=(14, 6))\n",
    "for country in top_countries:\n",
    "    country_data = df_pandas[df_pandas['country'] == country]\n",
    "    plt.plot(\n",
    "        country_data['year'], \n",
    "        country_data['primary_energy_consumption'],\n",
    "        label=country,\n",
    "        marker='o',\n",
    "        markersize=3\n",
    "    )\n",
    "\n",
    "plt.xlabel('Año')\n",
    "plt.ylabel('Consumo de Energía Primaria (TWh)')\n",
    "plt.title('Evolución del Consumo de Energía - Top 10 Países')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441d5c57",
   "metadata": {},
   "source": [
    "## 8. Feature Engineering con PySpark\n",
    "\n",
    "Creamos variables derivadas usando PySpark para transformaciones escalables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a231cb3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar funciones necesarias\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# 1. Ratio de energías renovables vs total\n",
    "df_features = df_spark.withColumn(\n",
    "    'renewable_ratio',\n",
    "    F.when(F.col('primary_energy_consumption') > 0,\n",
    "           F.col('renewables_consumption') / F.col('primary_energy_consumption')\n",
    "    ).otherwise(0)\n",
    ")\n",
    "\n",
    "# 2. Consumo per cápita\n",
    "df_features = df_features.withColumn(\n",
    "    'energy_per_capita',\n",
    "    F.when(F.col('population') > 0,\n",
    "           F.col('primary_energy_consumption') / F.col('population') * 1000000\n",
    "    ).otherwise(0)\n",
    ")\n",
    "\n",
    "# 3. Intensidad energética (energía por unidad de PIB)\n",
    "df_features = df_features.withColumn(\n",
    "    'energy_intensity',\n",
    "    F.when(F.col('gdp') > 0,\n",
    "           F.col('primary_energy_consumption') / F.col('gdp')\n",
    "    ).otherwise(0)\n",
    ")\n",
    "\n",
    "# 4. Índice de dependencia de combustibles fósiles\n",
    "df_features = df_features.withColumn(\n",
    "    'fossil_dependency_index',\n",
    "    F.when(F.col('primary_energy_consumption') > 0,\n",
    "           F.col('fossil_fuel_consumption') / F.col('primary_energy_consumption')\n",
    "    ).otherwise(0)\n",
    ")\n",
    "\n",
    "print(\"✓ Features básicos creados\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553072f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Variación año a año (usando Window Functions)\n",
    "windowSpec = Window.partitionBy('country').orderBy('year')\n",
    "\n",
    "df_features = df_features.withColumn(\n",
    "    'energy_yoy_change',\n",
    "    F.col('primary_energy_consumption') - F.lag('primary_energy_consumption').over(windowSpec)\n",
    ")\n",
    "\n",
    "df_features = df_features.withColumn(\n",
    "    'energy_yoy_pct_change',\n",
    "    F.when(F.lag('primary_energy_consumption').over(windowSpec) > 0,\n",
    "           ((F.col('primary_energy_consumption') - F.lag('primary_energy_consumption').over(windowSpec)) / \n",
    "            F.lag('primary_energy_consumption').over(windowSpec)) * 100\n",
    "    ).otherwise(0)\n",
    ")\n",
    "\n",
    "# 6. Categorización de países por nivel de desarrollo energético\n",
    "df_features = df_features.withColumn(\n",
    "    'energy_development_level',\n",
    "    F.when(F.col('energy_per_capita') >= 100, 'High')\n",
    "     .when(F.col('energy_per_capita') >= 50, 'Medium')\n",
    "     .when(F.col('energy_per_capita') > 0, 'Low')\n",
    "     .otherwise('Unknown')\n",
    ")\n",
    "\n",
    "print(\"✓ Features avanzados creados (variaciones temporales y categorización)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64fcc1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostrar resultados\n",
    "display(df_features.select(\n",
    "    'country', 'year', 'population', 'gdp',\n",
    "    'renewable_ratio', 'energy_per_capita', 'energy_intensity',\n",
    "    'fossil_dependency_index', 'energy_development_level'\n",
    ").orderBy(F.desc('year'), F.desc('energy_per_capita')).limit(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3adcfbb6",
   "metadata": {},
   "source": [
    "## 9. Imputación de Valores Faltantes\n",
    "\n",
    "Aplicamos diferentes estrategias de imputación para manejar valores faltantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff8a9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Imputer\n",
    "\n",
    "# Estrategia 1: Imputación por media (para variables continuas)\n",
    "imputer_mean = Imputer(\n",
    "    inputCols=['gdp', 'population', 'primary_energy_consumption'],\n",
    "    outputCols=['gdp_imputed', 'population_imputed', 'energy_imputed']\n",
    ").setStrategy('mean')\n",
    "\n",
    "# Estrategia 2: Imputación por mediana (más robusta a outliers)\n",
    "imputer_median = Imputer(\n",
    "    inputCols=['renewable_ratio', 'energy_per_capita'],\n",
    "    outputCols=['renewable_ratio_imputed', 'energy_per_capita_imputed']\n",
    ").setStrategy('median')\n",
    "\n",
    "# Aplicar imputación\n",
    "df_imputed = imputer_mean.fit(df_features).transform(df_features)\n",
    "df_imputed = imputer_median.fit(df_imputed).transform(df_imputed)\n",
    "\n",
    "print(\"✓ Imputación por media y mediana completada\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bfc50df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estrategia 3: Forward fill por país y año (para series temporales)\n",
    "windowSpec = Window.partitionBy('country').orderBy('year').rowsBetween(Window.unboundedPreceding, 0)\n",
    "\n",
    "df_imputed = df_imputed.withColumn(\n",
    "    'gdp_filled',\n",
    "    F.last('gdp', ignorenulls=True).over(windowSpec)\n",
    ")\n",
    "\n",
    "print(\"✓ Forward fill completado\")\n",
    "\n",
    "# Comparar estrategias de imputación\n",
    "display(df_imputed.select('country', 'year', 'gdp', 'gdp_imputed', 'gdp_filled').limit(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7651c36d",
   "metadata": {},
   "source": [
    "## 10. Encoding de Variables Categóricas\n",
    "\n",
    "Convertimos variables categóricas en representaciones numéricas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c2645c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
    "\n",
    "# String Indexer para 'country'\n",
    "country_indexer = StringIndexer(\n",
    "    inputCol='country',\n",
    "    outputCol='country_index',\n",
    "    handleInvalid='keep'\n",
    ")\n",
    "\n",
    "df_encoded = country_indexer.fit(df_imputed).transform(df_imputed)\n",
    "\n",
    "print(\"✓ String Indexer aplicado a 'country'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61648d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-Hot Encoding para nivel de desarrollo\n",
    "level_indexer = StringIndexer(\n",
    "    inputCol='energy_development_level',\n",
    "    outputCol='development_level_index',\n",
    "    handleInvalid='keep'\n",
    ")\n",
    "\n",
    "df_encoded = level_indexer.fit(df_encoded).transform(df_encoded)\n",
    "\n",
    "encoder = OneHotEncoder(\n",
    "    inputCols=['development_level_index'],\n",
    "    outputCols=['development_level_vec']\n",
    ")\n",
    "\n",
    "df_encoded = encoder.fit(df_encoded).transform(df_encoded)\n",
    "\n",
    "print(\"✓ One-Hot Encoding aplicado a 'energy_development_level'\")\n",
    "\n",
    "display(df_encoded.select(\n",
    "    'country', 'country_index', \n",
    "    'energy_development_level', 'development_level_index', 'development_level_vec'\n",
    ").limit(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fbf3887",
   "metadata": {},
   "source": [
    "## 11. Escalado y Normalización\n",
    "\n",
    "Aplicamos escalado estándar y normalización min-max a las features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cfa8d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler, StandardScaler, MinMaxScaler\n",
    "\n",
    "# Seleccionar features para escalado\n",
    "features_to_scale = [\n",
    "    'energy_per_capita_imputed',\n",
    "    'renewable_ratio_imputed',\n",
    "    'energy_intensity',\n",
    "    'fossil_dependency_index'\n",
    "]\n",
    "\n",
    "# Ensamblar features en un vector\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=features_to_scale,\n",
    "    outputCol='features_raw'\n",
    ")\n",
    "\n",
    "df_assembled = assembler.transform(df_encoded)\n",
    "\n",
    "print(\"✓ Features ensamblados en un vector\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25090856",
   "metadata": {},
   "outputs": [],
   "source": [
    "# StandardScaler (Z-score normalization: media=0, std=1)\n",
    "standard_scaler = StandardScaler(\n",
    "    inputCol='features_raw',\n",
    "    outputCol='features_standardized',\n",
    "    withMean=True,\n",
    "    withStd=True\n",
    ")\n",
    "\n",
    "scaler_model = standard_scaler.fit(df_assembled)\n",
    "df_scaled = scaler_model.transform(df_assembled)\n",
    "\n",
    "print(\"✓ StandardScaler aplicado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789efc5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MinMaxScaler (escalado a rango [0,1])\n",
    "minmax_scaler = MinMaxScaler(\n",
    "    inputCol='features_raw',\n",
    "    outputCol='features_normalized'\n",
    ")\n",
    "\n",
    "minmax_model = minmax_scaler.fit(df_assembled)\n",
    "df_scaled = minmax_model.transform(df_scaled)\n",
    "\n",
    "print(\"✓ MinMaxScaler aplicado\")\n",
    "\n",
    "display(df_scaled.select(\n",
    "    'country', 'year', \n",
    "    'features_raw', 'features_standardized', 'features_normalized'\n",
    ").limit(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1790d108",
   "metadata": {},
   "source": [
    "## 12. Selección de Features\n",
    "\n",
    "Analizamos correlaciones y filtramos features con baja varianza."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3b53c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.stat import Correlation\n",
    "\n",
    "# Correlación de Pearson\n",
    "correlation_matrix = Correlation.corr(df_scaled, 'features_raw', 'pearson')\n",
    "print(\"Matriz de Correlación:\")\n",
    "correlation_matrix.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10ab57a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VarianceThresholdSelector\n",
    "\n",
    "# Filtrar features con baja varianza\n",
    "variance_selector = VarianceThresholdSelector(\n",
    "    featuresCol='features_standardized',\n",
    "    outputCol='features_selected',\n",
    "    varianceThreshold=0.1\n",
    ")\n",
    "\n",
    "# Aplicar selector\n",
    "variance_model = variance_selector.fit(df_scaled)\n",
    "df_selected = variance_model.transform(df_scaled)\n",
    "\n",
    "print(f\"Features originales: {len(features_to_scale)}\")\n",
    "print(f\"Features después de filtrar por varianza: {len(variance_model.selectedFeatures)}\")\n",
    "print(f\"Features seleccionados: {variance_model.selectedFeatures}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7092fd0",
   "metadata": {},
   "source": [
    "## 13. Creación de Dataset Final para Modelado\n",
    "\n",
    "Seleccionamos las features finales y preparamos el dataset para machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6ddfbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seleccionar features finales\n",
    "final_features = [\n",
    "    'country', 'year', \n",
    "    'renewable_ratio_imputed',\n",
    "    'energy_per_capita_imputed',\n",
    "    'energy_intensity',\n",
    "    'fossil_dependency_index',\n",
    "    'energy_yoy_pct_change',\n",
    "    'country_index',\n",
    "    'development_level_index',\n",
    "    'features_standardized'\n",
    "]\n",
    "\n",
    "# Crear dataset final\n",
    "df_final = df_scaled.select(final_features)\n",
    "\n",
    "# Filtrar registros completos\n",
    "df_final = df_final.na.drop(subset=['renewable_ratio_imputed', 'energy_per_capita_imputed'])\n",
    "\n",
    "# Estadísticas del dataset final\n",
    "print(f\"Registros finales: {df_final.count():,}\")\n",
    "print(f\"Features: {len(final_features)}\")\n",
    "\n",
    "display(df_final.limit(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840313e7",
   "metadata": {},
   "source": [
    "## 14. Persistencia en Delta Lake\n",
    "\n",
    "Guardamos los features procesados en Delta Lake para su uso futuro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1b8d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir ruta para Delta Table\n",
    "delta_path = \"/delta/energy_features\"\n",
    "\n",
    "# Guardar como Delta Table con particionamiento\n",
    "df_final.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .partitionBy(\"year\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .save(delta_path)\n",
    "\n",
    "print(f\"✓ Features guardados en Delta Lake: {delta_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a985e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear tabla en el metastore\n",
    "df_final.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .partitionBy(\"year\") \\\n",
    "    .saveAsTable(\"energy_features_table\")\n",
    "\n",
    "print(\"✓ Tabla 'energy_features_table' creada exitosamente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0afccf3a",
   "metadata": {},
   "source": [
    "## 15. Verificación de Delta Table\n",
    "\n",
    "Verificamos que los datos se guardaron correctamente y exploramos el historial de versiones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f628ba19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leer desde Delta Table\n",
    "df_from_delta = spark.read.format(\"delta\").load(delta_path)\n",
    "\n",
    "print(\"Esquema de la tabla Delta:\")\n",
    "df_from_delta.printSchema()\n",
    "\n",
    "print(f\"\\nRegistros en Delta Table: {df_from_delta.count():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e8bb3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contar registros por año\n",
    "print(\"Registros por año:\")\n",
    "display(df_from_delta.groupBy(\"year\").count().orderBy(\"year\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c3be85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar historial de versiones\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "delta_table = DeltaTable.forPath(spark, delta_path)\n",
    "print(\"Historial de versiones de la tabla Delta:\")\n",
    "display(delta_table.history())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc8281a",
   "metadata": {},
   "source": [
    "## 16. Control de Data Drift\n",
    "\n",
    "Analizamos cambios en la distribución de datos a lo largo del tiempo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a8ff90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular estadísticas por período\n",
    "df_with_period = df_final.withColumn(\n",
    "    'period',\n",
    "    F.when(F.col('year') < 2000, 'pre_2000')\n",
    "     .when(F.col('year') < 2010, '2000-2009')\n",
    "     .when(F.col('year') < 2020, '2010-2019')\n",
    "     .otherwise('2020+')\n",
    ")\n",
    "\n",
    "# Comparar distribuciones por período\n",
    "drift_analysis = df_with_period.groupBy('period').agg(\n",
    "    F.mean('energy_per_capita_imputed').alias('avg_energy_per_capita'),\n",
    "    F.stddev('energy_per_capita_imputed').alias('std_energy_per_capita'),\n",
    "    F.mean('renewable_ratio_imputed').alias('avg_renewable_ratio'),\n",
    "    F.stddev('renewable_ratio_imputed').alias('std_renewable_ratio'),\n",
    "    F.count('*').alias('record_count')\n",
    ").orderBy('period')\n",
    "\n",
    "print(\"Análisis de Drift por Período:\")\n",
    "display(drift_analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa698dda",
   "metadata": {},
   "source": [
    "## 17. Validación de Calidad de Datos\n",
    "\n",
    "Generamos un reporte de calidad de los features procesados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5ce3ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, isnan, when, count\n",
    "\n",
    "# Función de validación de calidad\n",
    "def data_quality_check(df, columns_to_check):\n",
    "    \"\"\"\n",
    "    Genera reporte de calidad de datos\n",
    "    \"\"\"\n",
    "    quality_metrics = []\n",
    "    \n",
    "    for column in columns_to_check:\n",
    "        # Conteo de valores nulos\n",
    "        null_count = df.filter(col(column).isNull()).count()\n",
    "        \n",
    "        # Conteo de valores negativos (si es numérico)\n",
    "        negative_count = df.filter(col(column) < 0).count()\n",
    "        \n",
    "        # Valores únicos\n",
    "        distinct_count = df.select(column).distinct().count()\n",
    "        \n",
    "        quality_metrics.append({\n",
    "            'column': column,\n",
    "            'null_count': null_count,\n",
    "            'null_percentage': (null_count / df.count()) * 100,\n",
    "            'negative_count': negative_count,\n",
    "            'distinct_values': distinct_count\n",
    "        })\n",
    "    \n",
    "    return spark.createDataFrame(quality_metrics)\n",
    "\n",
    "# Aplicar validación\n",
    "columns_to_validate = [\n",
    "    'renewable_ratio_imputed',\n",
    "    'energy_per_capita_imputed',\n",
    "    'energy_intensity',\n",
    "    'fossil_dependency_index'\n",
    "]\n",
    "\n",
    "quality_report = data_quality_check(df_final, columns_to_validate)\n",
    "print(\"Reporte de Calidad de Datos:\")\n",
    "display(quality_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64179d93",
   "metadata": {},
   "source": [
    "## 18. Documentación de Features\n",
    "\n",
    "Creamos un diccionario de features para documentar las transformaciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58658122",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Crear diccionario de features\n",
    "feature_dictionary = {\n",
    "    'renewable_ratio': {\n",
    "        'description': 'Ratio de energías renovables vs consumo total de energía',\n",
    "        'formula': 'renewables_consumption / primary_energy_consumption',\n",
    "        'range': '[0, 1]',\n",
    "        'type': 'numeric - continuous',\n",
    "        'imputation': 'median'\n",
    "    },\n",
    "    'energy_per_capita': {\n",
    "        'description': 'Consumo de energía por persona en kWh',\n",
    "        'formula': '(primary_energy_consumption / population) * 1000000',\n",
    "        'range': '[0, +inf]',\n",
    "        'type': 'numeric - continuous',\n",
    "        'imputation': 'median'\n",
    "    },\n",
    "    'energy_intensity': {\n",
    "        'description': 'Consumo de energía por unidad de PIB',\n",
    "        'formula': 'primary_energy_consumption / gdp',\n",
    "        'range': '[0, +inf]',\n",
    "        'type': 'numeric - continuous',\n",
    "        'imputation': 'mean'\n",
    "    },\n",
    "    'fossil_dependency_index': {\n",
    "        'description': 'Índice de dependencia de combustibles fósiles',\n",
    "        'formula': 'fossil_fuel_consumption / primary_energy_consumption',\n",
    "        'range': '[0, 1]',\n",
    "        'type': 'numeric - continuous',\n",
    "        'imputation': 'mean'\n",
    "    },\n",
    "    'energy_development_level': {\n",
    "        'description': 'Nivel de desarrollo energético del país',\n",
    "        'formula': 'Categorización basada en energy_per_capita',\n",
    "        'categories': ['Low', 'Medium', 'High', 'Unknown'],\n",
    "        'type': 'categorical',\n",
    "        'encoding': 'one-hot'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Convertir a JSON y mostrar\n",
    "feature_dict_json = json.dumps(feature_dictionary, indent=2)\n",
    "print(\"Diccionario de Features:\")\n",
    "print(feature_dict_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef82295d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar en DBFS\n",
    "dbutils.fs.put(\n",
    "    \"/FileStore/energy_features_dictionary.json\",\n",
    "    feature_dict_json,\n",
    "    overwrite=True\n",
    ")\n",
    "\n",
    "print(\"✓ Diccionario de features guardado en DBFS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02c6fcf",
   "metadata": {},
   "source": [
    "## 19. Pipeline Completo Reproducible\n",
    "\n",
    "Creamos una función que encapsula todo el pipeline de feature engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621b7211",
   "metadata": {},
   "outputs": [],
   "source": [
    "def energy_feature_pipeline(input_path, output_path, imputation_strategy='median'):\n",
    "    \"\"\"\n",
    "    Pipeline completo de feature engineering para datos de energía\n",
    "    \n",
    "    Args:\n",
    "        input_path: Ruta al archivo CSV de entrada\n",
    "        output_path: Ruta de salida para Delta Table\n",
    "        imputation_strategy: Estrategia de imputación ('mean', 'median')\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame con features procesados\n",
    "    \"\"\"\n",
    "    from pyspark.ml.feature import Imputer, StringIndexer, VectorAssembler, StandardScaler\n",
    "    \n",
    "    # 1. Carga de datos\n",
    "    df = spark.read.csv(input_path, header=True, inferSchema=True)\n",
    "    print(f\"✓ Datos cargados: {df.count():,} registros\")\n",
    "    \n",
    "    # 2. Feature Engineering\n",
    "    df = df.withColumn(\n",
    "        'renewable_ratio',\n",
    "        F.when(F.col('primary_energy_consumption') > 0,\n",
    "               F.col('renewables_consumption') / F.col('primary_energy_consumption')\n",
    "        ).otherwise(0)\n",
    "    )\n",
    "    \n",
    "    df = df.withColumn(\n",
    "        'energy_per_capita',\n",
    "        F.when(F.col('population') > 0,\n",
    "               F.col('primary_energy_consumption') / F.col('population') * 1000000\n",
    "        ).otherwise(0)\n",
    "    )\n",
    "    \n",
    "    df = df.withColumn(\n",
    "        'energy_intensity',\n",
    "        F.when(F.col('gdp') > 0,\n",
    "               F.col('primary_energy_consumption') / F.col('gdp')\n",
    "        ).otherwise(0)\n",
    "    )\n",
    "    \n",
    "    df = df.withColumn(\n",
    "        'fossil_dependency_index',\n",
    "        F.when(F.col('primary_energy_consumption') > 0,\n",
    "               F.col('fossil_fuel_consumption') / F.col('primary_energy_consumption')\n",
    "        ).otherwise(0)\n",
    "    )\n",
    "    \n",
    "    print(\"✓ Features derivados creados\")\n",
    "    \n",
    "    # 3. Imputación\n",
    "    imputer = Imputer(\n",
    "        inputCols=['renewable_ratio', 'energy_per_capita', 'energy_intensity'],\n",
    "        outputCols=['renewable_ratio_imputed', 'energy_per_capita_imputed', 'energy_intensity_imputed']\n",
    "    ).setStrategy(imputation_strategy)\n",
    "    \n",
    "    df = imputer.fit(df).transform(df)\n",
    "    print(f\"✓ Imputación completada (estrategia: {imputation_strategy})\")\n",
    "    \n",
    "    # 4. Encoding\n",
    "    indexer = StringIndexer(inputCol='country', outputCol='country_index', handleInvalid='keep')\n",
    "    df = indexer.fit(df).transform(df)\n",
    "    print(\"✓ Encoding completado\")\n",
    "    \n",
    "    # 5. Escalado\n",
    "    assembler = VectorAssembler(\n",
    "        inputCols=['renewable_ratio_imputed', 'energy_per_capita_imputed', 'energy_intensity_imputed'],\n",
    "        outputCol='features_raw'\n",
    "    )\n",
    "    df = assembler.transform(df)\n",
    "    \n",
    "    scaler = StandardScaler(inputCol='features_raw', outputCol='features_scaled', withMean=True, withStd=True)\n",
    "    df = scaler.fit(df).transform(df)\n",
    "    print(\"✓ Escalado completado\")\n",
    "    \n",
    "    # 6. Persistencia\n",
    "    df.write.format(\"delta\").mode(\"overwrite\").partitionBy(\"year\").save(output_path)\n",
    "    print(f\"✓ Datos guardados en: {output_path}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "print(\"✓ Función de pipeline definida\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ea7c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejecutar pipeline\n",
    "result_df = energy_feature_pipeline(\n",
    "    input_path=\"/FileStore/tables/owid-energy-data.csv\",\n",
    "    output_path=\"/delta/energy_features_pipeline\",\n",
    "    imputation_strategy='median'\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"PIPELINE COMPLETADO EXITOSAMENTE\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0791a18",
   "metadata": {},
   "source": [
    "## 20. Reporte Final de Calidad\n",
    "\n",
    "Generamos un reporte final con métricas de calidad del dataset procesado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3642b088",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_quality_report(df):\n",
    "    \"\"\"Genera reporte de calidad de features\"\"\"\n",
    "    report = {\n",
    "        'total_records': df.count(),\n",
    "        'total_features': len(df.columns),\n",
    "        'numeric_features': len([f for f in df.schema.fields \n",
    "                                if isinstance(f.dataType, (DoubleType, FloatType))]),\n",
    "        'categorical_features': len([f for f in df.schema.fields \n",
    "                                    if isinstance(f.dataType, StringType)])\n",
    "    }\n",
    "    return report\n",
    "\n",
    "quality_metrics = generate_quality_report(df_final)\n",
    "print(\"Reporte de Calidad Final:\")\n",
    "print(json.dumps(quality_metrics, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445aa552",
   "metadata": {},
   "source": [
    "## Conclusión\n",
    "\n",
    "¡Felicitaciones! Has completado exitosamente el laboratorio de Feature Engineering y Exploración de Datos.\n",
    "\n",
    "### Habilidades Adquiridas:\n",
    "\n",
    "✅ Análisis exploratorio completo con Pandas y Spark  \n",
    "✅ Creación de features derivados con significado de negocio  \n",
    "✅ Implementación de pipelines reproducibles de transformación  \n",
    "✅ Manejo de valores faltantes con diferentes estrategias  \n",
    "✅ Encoding y escalado de features  \n",
    "✅ Persistencia en Delta Lake con particionamiento  \n",
    "✅ Validación de calidad y detección de drift  \n",
    "\n",
    "### Próximos Pasos:\n",
    "\n",
    "- Construcción de modelos de ML con los features creados\n",
    "- Integración con MLflow para tracking de experimentos\n",
    "- Deployment de modelos en producción\n",
    "- Monitoreo de performance y drift en producción\n",
    "\n",
    "**¡Excelente trabajo!** Estos features están listos para ser utilizados en modelos de machine learning."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
