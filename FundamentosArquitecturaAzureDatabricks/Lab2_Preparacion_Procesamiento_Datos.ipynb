{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4010ef6",
   "metadata": {},
   "source": [
    "## 1. ConfiguraciÃ³n Inicial\n",
    "\n",
    "Verificamos el ambiente de Databricks y las versiones de componentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078fa56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar versiÃ³n de Spark\n",
    "print(f\"Spark Version: {spark.version}\")\n",
    "print(f\"Python Version: {spark.conf.get('spark.executorEnv.PYTHONHASHSEED', 'Not set')}\")\n",
    "\n",
    "# Importar librerÃ­as necesarias\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from delta.tables import DeltaTable\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "print(\"\\nâœ“ LibrerÃ­as importadas correctamente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd6cb61",
   "metadata": {},
   "source": [
    "## 2. Carga y ExploraciÃ³n de Datos\n",
    "\n",
    "Cargamos el dataset de energÃ­a mundial (OWID) y exploramos su estructura."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9b4720",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar datos desde CSV\n",
    "df = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .load(\"/workspaces/AzureDatabricksWorkshop/FundamentosArquitecturaAzureDatabricks/owid-energy-data.csv\")\n",
    "\n",
    "# InformaciÃ³n bÃ¡sica\n",
    "print(f\"Total de registros: {df.count():,}\")\n",
    "print(f\"Total de columnas: {len(df.columns)}\")\n",
    "print(f\"\\nPrimeras columnas: {df.columns[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59401e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostrar esquema del DataFrame\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe54c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EstadÃ­sticas descriptivas\n",
    "display(df.select(\n",
    "    \"country\", \n",
    "    \"year\", \n",
    "    \"population\", \n",
    "    \"gdp\", \n",
    "    \"primary_energy_consumption\"\n",
    ").describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69a164d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Muestra de datos\n",
    "display(df.select(\n",
    "    \"country\", \n",
    "    \"year\", \n",
    "    \"population\", \n",
    "    \"gdp\",\n",
    "    \"primary_energy_consumption\",\n",
    "    \"renewables_consumption\"\n",
    ").limit(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199113a5",
   "metadata": {},
   "source": [
    "## 3. AnÃ¡lisis de Calidad de Datos\n",
    "\n",
    "Identificamos problemas de calidad: valores nulos, duplicados, outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177bb9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AnÃ¡lisis de valores nulos por columna\n",
    "null_counts = df.select([\n",
    "    F.count(F.when(F.col(c).isNull(), c)).alias(c) \n",
    "    for c in df.columns\n",
    "])\n",
    "\n",
    "# Transponer para mejor visualizaciÃ³n\n",
    "null_data = [(col, null_counts.select(col).first()[0]) for col in null_counts.columns]\n",
    "null_df = spark.createDataFrame(null_data, [\"column\", \"null_count\"])\n",
    "null_df_filtered = null_df.filter(F.col(\"null_count\") > 0).orderBy(F.desc(\"null_count\"))\n",
    "\n",
    "display(null_df_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72044712",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detectar duplicados por paÃ­s y aÃ±o\n",
    "duplicates = df.groupBy(\"country\", \"year\").count().filter(F.col(\"count\") > 1)\n",
    "duplicate_count = duplicates.count()\n",
    "\n",
    "print(f\"Registros duplicados (country + year): {duplicate_count}\")\n",
    "\n",
    "if duplicate_count > 0:\n",
    "    display(duplicates.orderBy(F.desc(\"count\")).limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b374c640",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rango de aÃ±os en el dataset\n",
    "year_stats = df.select(\n",
    "    F.min(\"year\").alias(\"min_year\"),\n",
    "    F.max(\"year\").alias(\"max_year\"),\n",
    "    F.count(F.col(\"year\").isNotNull()).alias(\"valid_years\")\n",
    ")\n",
    "\n",
    "display(year_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9175e78",
   "metadata": {},
   "source": [
    "## 4. Limpieza de Datos con PySpark\n",
    "\n",
    "Aplicamos transformaciones para limpiar y preparar los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309bd409",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Eliminar duplicados\n",
    "df_clean = df.dropDuplicates([\"country\", \"year\"])\n",
    "\n",
    "print(f\"Registros originales: {df.count():,}\")\n",
    "print(f\"Registros despuÃ©s de eliminar duplicados: {df_clean.count():,}\")\n",
    "print(f\"Duplicados eliminados: {df.count() - df_clean.count():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07752bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Filtrar registros vÃ¡lidos\n",
    "df_clean = df_clean.filter(\n",
    "    (F.col(\"country\").isNotNull()) & \n",
    "    (F.col(\"year\").isNotNull()) &\n",
    "    (F.col(\"year\") >= 1900) &\n",
    "    (F.col(\"year\") <= 2025)\n",
    ")\n",
    "\n",
    "print(f\"Registros vÃ¡lidos: {df_clean.count():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22145448",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. ImputaciÃ³n de valores nulos - Consumos energÃ©ticos con 0\n",
    "energy_cols = [\n",
    "    \"primary_energy_consumption\",\n",
    "    \"renewables_consumption\",\n",
    "    \"fossil_fuel_consumption\",\n",
    "    \"coal_consumption\",\n",
    "    \"gas_consumption\",\n",
    "    \"oil_consumption\"\n",
    "]\n",
    "\n",
    "for col in energy_cols:\n",
    "    if col in df_clean.columns:\n",
    "        df_clean = df_clean.withColumn(\n",
    "            col,\n",
    "            F.when(F.col(col).isNull(), 0).otherwise(F.col(col))\n",
    "        )\n",
    "\n",
    "print(f\"âœ“ Valores nulos imputados en {len([c for c in energy_cols if c in df_clean.columns])} columnas de energÃ­a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369856eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. ImputaciÃ³n con Imputer de ML para variables demogrÃ¡ficas\n",
    "from pyspark.ml.feature import Imputer\n",
    "\n",
    "demographic_cols = [\"population\", \"gdp\"]\n",
    "output_cols = [f\"{col}_imputed\" for col in demographic_cols]\n",
    "\n",
    "imputer = Imputer(\n",
    "    inputCols=demographic_cols,\n",
    "    outputCols=output_cols\n",
    ").setStrategy(\"median\")\n",
    "\n",
    "# Fit y transform\n",
    "imputer_model = imputer.fit(df_clean)\n",
    "df_clean = imputer_model.transform(df_clean)\n",
    "\n",
    "print(\"âœ“ ImputaciÃ³n con mediana completada para poblaciÃ³n y GDP\")\n",
    "display(df_clean.select(\"country\", \"year\", \"population\", \"population_imputed\", \"gdp\", \"gdp_imputed\").limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86999855",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Normalizar nombres de paÃ­ses\n",
    "df_clean = df_clean.withColumn(\n",
    "    \"country\",\n",
    "    F.trim(F.upper(F.col(\"country\")))\n",
    ")\n",
    "\n",
    "# Verificar normalizaciÃ³n\n",
    "print(\"Muestra de paÃ­ses normalizados:\")\n",
    "display(df_clean.select(\"country\").distinct().orderBy(\"country\").limit(15))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e692be48",
   "metadata": {},
   "source": [
    "## 5. Transformaciones y Feature Engineering\n",
    "\n",
    "Creamos nuevas variables derivadas para anÃ¡lisis avanzado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae98eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Ratio de energÃ­as renovables\n",
    "df_transformed = df_clean.withColumn(\n",
    "    \"renewable_ratio\",\n",
    "    F.when(F.col(\"primary_energy_consumption\") > 0,\n",
    "           F.col(\"renewables_consumption\") / F.col(\"primary_energy_consumption\")\n",
    "    ).otherwise(0)\n",
    ")\n",
    "\n",
    "# 2. EnergÃ­a per cÃ¡pita\n",
    "df_transformed = df_transformed.withColumn(\n",
    "    \"energy_per_capita\",\n",
    "    F.when(F.col(\"population_imputed\") > 0,\n",
    "           F.col(\"primary_energy_consumption\") / F.col(\"population_imputed\") * 1000000\n",
    "    ).otherwise(0)\n",
    ")\n",
    "\n",
    "# 3. Intensidad energÃ©tica (energÃ­a/GDP)\n",
    "df_transformed = df_transformed.withColumn(\n",
    "    \"energy_intensity\",\n",
    "    F.when(F.col(\"gdp_imputed\") > 0,\n",
    "           F.col(\"primary_energy_consumption\") / F.col(\"gdp_imputed\")\n",
    "    ).otherwise(0)\n",
    ")\n",
    "\n",
    "# 4. Ratio de combustibles fÃ³siles\n",
    "df_transformed = df_transformed.withColumn(\n",
    "    \"fossil_ratio\",\n",
    "    F.when(F.col(\"primary_energy_consumption\") > 0,\n",
    "           F.col(\"fossil_fuel_consumption\") / F.col(\"primary_energy_consumption\")\n",
    "    ).otherwise(0)\n",
    ")\n",
    "\n",
    "print(\"âœ“ Features calculados: renewable_ratio, energy_per_capita, energy_intensity, fossil_ratio\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9abf15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. CategorizaciÃ³n temporal\n",
    "df_transformed = df_transformed.withColumn(\n",
    "    \"decade\",\n",
    "    (F.floor(F.col(\"year\") / 10) * 10).cast(IntegerType())\n",
    ")\n",
    "\n",
    "# 6. Niveles de consumo\n",
    "df_transformed = df_transformed.withColumn(\n",
    "    \"consumption_level\",\n",
    "    F.when(F.col(\"energy_per_capita\") >= 100, \"High\")\n",
    "     .when(F.col(\"energy_per_capita\") >= 50, \"Medium\")\n",
    "     .when(F.col(\"energy_per_capita\") > 0, \"Low\")\n",
    "     .otherwise(\"Unknown\")\n",
    ")\n",
    "\n",
    "print(\"âœ“ CategorÃ­as creadas: decade, consumption_level\")\n",
    "\n",
    "# Visualizar distribuciÃ³n de niveles de consumo\n",
    "display(df_transformed.groupBy(\"consumption_level\").count().orderBy(F.desc(\"count\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd55e5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar datos transformados\n",
    "display(df_transformed.select(\n",
    "    \"country\",\n",
    "    \"year\",\n",
    "    \"decade\",\n",
    "    \"renewable_ratio\",\n",
    "    \"energy_per_capita\",\n",
    "    \"energy_intensity\",\n",
    "    \"consumption_level\"\n",
    ").orderBy(F.desc(\"energy_per_capita\")).limit(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2584a8",
   "metadata": {},
   "source": [
    "## 6. Agregaciones y AnÃ¡lisis\n",
    "\n",
    "Creamos vistas agregadas para anÃ¡lisis temporal y por regiÃ³n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a560ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AgregaciÃ³n por dÃ©cada\n",
    "df_by_decade = df_transformed.groupBy(\"decade\").agg(\n",
    "    F.avg(\"primary_energy_consumption\").alias(\"avg_energy_consumption\"),\n",
    "    F.avg(\"renewable_ratio\").alias(\"avg_renewable_ratio\"),\n",
    "    F.avg(\"fossil_ratio\").alias(\"avg_fossil_ratio\"),\n",
    "    F.sum(\"population_imputed\").alias(\"total_population\"),\n",
    "    F.count(\"*\").alias(\"record_count\")\n",
    ").orderBy(\"decade\")\n",
    "\n",
    "print(\"Tendencias por dÃ©cada:\")\n",
    "display(df_by_decade)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f493d20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 10 paÃ­ses por consumo energÃ©tico (Ãºltimos datos disponibles)\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "window_spec = Window.partitionBy(\"country\").orderBy(F.desc(\"year\"))\n",
    "\n",
    "df_latest = df_transformed.withColumn(\n",
    "    \"row_num\",\n",
    "    F.row_number().over(window_spec)\n",
    ").filter(F.col(\"row_num\") == 1)\n",
    "\n",
    "top_consumers = df_latest.select(\n",
    "    \"country\",\n",
    "    \"year\",\n",
    "    \"primary_energy_consumption\",\n",
    "    \"renewable_ratio\",\n",
    "    \"energy_per_capita\"\n",
    ").orderBy(F.desc(\"primary_energy_consumption\")).limit(10)\n",
    "\n",
    "print(\"Top 10 paÃ­ses por consumo energÃ©tico total:\")\n",
    "display(top_consumers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43dc8a02",
   "metadata": {},
   "source": [
    "## 7. ValidaciÃ³n de Calidad de Datos\n",
    "\n",
    "Implementamos reglas de validaciÃ³n automatizadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5038162d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_data_quality(df, rules):\n",
    "    \"\"\"\n",
    "    Valida reglas de calidad en el DataFrame\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame de Spark\n",
    "        rules: Diccionario con reglas de validaciÃ³n\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame con resultados de validaciÃ³n\n",
    "    \"\"\"\n",
    "    validation_results = []\n",
    "    total = df.count()\n",
    "    \n",
    "    for rule_name, rule_condition in rules.items():\n",
    "        passed = df.filter(rule_condition).count()\n",
    "        failed = total - passed\n",
    "        \n",
    "        validation_results.append({\n",
    "            \"rule\": rule_name,\n",
    "            \"total_records\": total,\n",
    "            \"passed\": passed,\n",
    "            \"failed\": failed,\n",
    "            \"pass_rate\": round((passed / total * 100) if total > 0 else 0, 2)\n",
    "        })\n",
    "    \n",
    "    return spark.createDataFrame(validation_results)\n",
    "\n",
    "# Definir reglas de validaciÃ³n\n",
    "validation_rules = {\n",
    "    \"year_in_range\": (F.col(\"year\") >= 1900) & (F.col(\"year\") <= 2025),\n",
    "    \"population_positive\": F.col(\"population_imputed\") >= 0,\n",
    "    \"energy_non_negative\": F.col(\"primary_energy_consumption\") >= 0,\n",
    "    \"renewable_ratio_valid\": (F.col(\"renewable_ratio\") >= 0) & (F.col(\"renewable_ratio\") <= 1),\n",
    "    \"country_not_null\": F.col(\"country\").isNotNull(),\n",
    "    \"decade_consistent\": F.col(\"decade\") == (F.floor(F.col(\"year\") / 10) * 10)\n",
    "}\n",
    "\n",
    "# Ejecutar validaciÃ³n\n",
    "validation_report = validate_data_quality(df_transformed, validation_rules)\n",
    "\n",
    "print(\"Reporte de Calidad de Datos:\")\n",
    "display(validation_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa33f357",
   "metadata": {},
   "source": [
    "## 8. Delta Lake - Guardar Datos con ACID\n",
    "\n",
    "Escribimos los datos transformados en formato Delta Lake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d51304",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir ruta para Delta Lake\n",
    "delta_path = \"/tmp/delta/energy_processed\"\n",
    "\n",
    "# Escribir datos como tabla Delta\n",
    "df_transformed.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .save(delta_path)\n",
    "\n",
    "print(f\"âœ“ Tabla Delta creada en: {delta_path}\")\n",
    "print(f\"âœ“ Registros guardados: {df_transformed.count():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25aced9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar que podemos leer la tabla Delta\n",
    "df_delta = spark.read.format(\"delta\").load(delta_path)\n",
    "\n",
    "print(f\"Registros en Delta Lake: {df_delta.count():,}\")\n",
    "display(df_delta.limit(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53f181c",
   "metadata": {},
   "source": [
    "## 9. Versionado y Time Travel\n",
    "\n",
    "Exploramos las capacidades de versionado de Delta Lake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a929bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener tabla Delta\n",
    "delta_table = DeltaTable.forPath(spark, delta_path)\n",
    "\n",
    "# Ver historial de versiones\n",
    "history = delta_table.history()\n",
    "\n",
    "print(\"Historial de versiones:\")\n",
    "display(history.select(\"version\", \"timestamp\", \"operation\", \"operationMetrics\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c49057",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hacer una actualizaciÃ³n para crear nueva versiÃ³n\n",
    "delta_table.update(\n",
    "    condition=\"year = 2020 AND country = 'UNITED STATES'\",\n",
    "    set={\"consumption_level\": \"'Updated'\"}\n",
    ")\n",
    "\n",
    "print(\"âœ“ Registros actualizados\")\n",
    "\n",
    "# Verificar nueva versiÃ³n\n",
    "history_updated = delta_table.history()\n",
    "print(f\"\\nTotal de versiones: {history_updated.count()}\")\n",
    "display(history_updated.select(\"version\", \"timestamp\", \"operation\").limit(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00faab97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time Travel: Leer versiÃ³n anterior\n",
    "df_version_0 = spark.read \\\n",
    "    .format(\"delta\") \\\n",
    "    .option(\"versionAsOf\", 0) \\\n",
    "    .load(delta_path)\n",
    "\n",
    "df_version_1 = spark.read \\\n",
    "    .format(\"delta\") \\\n",
    "    .option(\"versionAsOf\", 1) \\\n",
    "    .load(delta_path)\n",
    "\n",
    "print(f\"Registros en versiÃ³n 0: {df_version_0.count():,}\")\n",
    "print(f\"Registros en versiÃ³n 1: {df_version_1.count():,}\")\n",
    "\n",
    "# Comparar registros de Estados Unidos\n",
    "print(\"\\nComparaciÃ³n de versiones (USA, 2020):\")\n",
    "display(df_version_0.filter((F.col(\"country\") == \"UNITED STATES\") & (F.col(\"year\") == 2020)).select(\"country\", \"year\", \"consumption_level\"))\n",
    "display(df_version_1.filter((F.col(\"country\") == \"UNITED STATES\") & (F.col(\"year\") == 2020)).select(\"country\", \"year\", \"consumption_level\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d155093f",
   "metadata": {},
   "source": [
    "## 10. Operaciones ACID: Merge (Upsert)\n",
    "\n",
    "Demostramos la operaciÃ³n MERGE para actualizar e insertar datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81899be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear datos nuevos para MERGE\n",
    "new_data = spark.createDataFrame([\n",
    "    (\"BRAZIL\", 2024, 215000000.0, 5000.0, 0.45, 23.36, 0.55, \"High\", 2020),\n",
    "    (\"INDIA\", 2024, 1400000000.0, 6000.0, 0.35, 4.29, 0.65, \"Low\", 2020),\n",
    "    (\"GERMANY\", 2024, 83000000.0, 3500.0, 0.50, 42.17, 0.50, \"High\", 2020)\n",
    "], [\"country\", \"year\", \"population_imputed\", \"primary_energy_consumption\", \n",
    "    \"renewable_ratio\", \"energy_per_capita\", \"fossil_ratio\", \"consumption_level\", \"decade\"])\n",
    "\n",
    "print(\"Datos nuevos a fusionar:\")\n",
    "display(new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504678f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejecutar MERGE (Upsert)\n",
    "delta_table.alias(\"target\").merge(\n",
    "    new_data.alias(\"source\"),\n",
    "    \"target.country = source.country AND target.year = source.year\"\n",
    ").whenMatchedUpdateAll() \\\n",
    " .whenNotMatchedInsertAll() \\\n",
    " .execute()\n",
    "\n",
    "print(\"âœ“ OperaciÃ³n MERGE completada\")\n",
    "\n",
    "# Verificar resultados\n",
    "df_merged = spark.read.format(\"delta\").load(delta_path)\n",
    "print(f\"\\nTotal de registros despuÃ©s de MERGE: {df_merged.count():,}\")\n",
    "\n",
    "# Mostrar registros fusionados\n",
    "display(df_merged.filter(\n",
    "    (F.col(\"country\").isin([\"BRAZIL\", \"INDIA\", \"GERMANY\"])) & \n",
    "    (F.col(\"year\") == 2024)\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef7987a",
   "metadata": {},
   "source": [
    "## 11. OptimizaciÃ³n con OPTIMIZE y Z-ORDER\n",
    "\n",
    "Optimizamos la tabla Delta para mejorar performance de consultas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0c458b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIMIZE: Compactar archivos pequeÃ±os\n",
    "spark.sql(f\"\"\"\n",
    "    OPTIMIZE delta.`{delta_path}`\n",
    "\"\"\")\n",
    "\n",
    "print(\"âœ“ OPTIMIZE completado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33db0846",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Z-ORDER: Optimizar para queries frecuentes\n",
    "spark.sql(f\"\"\"\n",
    "    OPTIMIZE delta.`{delta_path}`\n",
    "    ZORDER BY (country, year)\n",
    "\"\"\")\n",
    "\n",
    "print(\"âœ“ Z-ORDER completado en columnas: country, year\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4bae70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ver detalles de la tabla optimizada\n",
    "detail_df = spark.sql(f\"DESCRIBE DETAIL delta.`{delta_path}`\")\n",
    "display(detail_df.select(\"format\", \"numFiles\", \"sizeInBytes\", \"properties\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c2db1c",
   "metadata": {},
   "source": [
    "## 12. Particionamiento\n",
    "\n",
    "Creamos una tabla particionada para mejorar performance de queries con filtros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2a254f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear tabla particionada por dÃ©cada\n",
    "delta_path_partitioned = \"/tmp/delta/energy_partitioned\"\n",
    "\n",
    "df_transformed.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .partitionBy(\"decade\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .save(delta_path_partitioned)\n",
    "\n",
    "print(f\"âœ“ Tabla particionada creada en: {delta_path_partitioned}\")\n",
    "print(\"âœ“ Particionada por: decade\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4d6ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar estructura de particiones\n",
    "partitioned_detail = spark.sql(f\"DESCRIBE DETAIL delta.`{delta_path_partitioned}`\")\n",
    "display(partitioned_detail.select(\"format\", \"partitionColumns\", \"numFiles\", \"sizeInBytes\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54be23b",
   "metadata": {},
   "source": [
    "## 13. Pipeline ETL Completo - FunciÃ³n Reutilizable\n",
    "\n",
    "Encapsulamos todo el proceso ETL en una funciÃ³n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691c2c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def etl_pipeline(source_path, target_path, partition_cols=None):\n",
    "    \"\"\"\n",
    "    Pipeline ETL completo: Extract, Transform, Load\n",
    "    \n",
    "    Args:\n",
    "        source_path: Ruta al archivo fuente\n",
    "        target_path: Ruta de destino para Delta Table\n",
    "        partition_cols: Lista de columnas para particionar\n",
    "    \n",
    "    Returns:\n",
    "        NÃºmero de registros procesados\n",
    "    \"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"INICIANDO PIPELINE ETL\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # EXTRACT\n",
    "    print(\"\\n[1/5] EXTRACT - Cargando datos...\")\n",
    "    df_raw = spark.read.format(\"csv\") \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .option(\"inferSchema\", \"true\") \\\n",
    "        .load(source_path)\n",
    "    \n",
    "    initial_count = df_raw.count()\n",
    "    print(f\"âœ“ Registros cargados: {initial_count:,}\")\n",
    "    \n",
    "    # TRANSFORM - Limpieza\n",
    "    print(\"\\n[2/5] TRANSFORM - Limpiando datos...\")\n",
    "    df_clean = df_raw.dropDuplicates([\"country\", \"year\"]) \\\n",
    "        .filter((F.col(\"country\").isNotNull()) & (F.col(\"year\").isNotNull()))\n",
    "    \n",
    "    # Imputar valores nulos\n",
    "    for col_name in [\"population\", \"gdp\", \"primary_energy_consumption\"]:\n",
    "        if col_name in df_clean.columns:\n",
    "            df_clean = df_clean.withColumn(\n",
    "                col_name,\n",
    "                F.when(F.col(col_name).isNull(), 0).otherwise(F.col(col_name))\n",
    "            )\n",
    "    \n",
    "    clean_count = df_clean.count()\n",
    "    print(f\"âœ“ Registros despuÃ©s de limpieza: {clean_count:,} ({initial_count - clean_count:,} eliminados)\")\n",
    "    \n",
    "    # TRANSFORM - Features\n",
    "    print(\"\\n[3/5] TRANSFORM - Creando features...\")\n",
    "    df_transformed = df_clean \\\n",
    "        .withColumn(\"renewable_ratio\",\n",
    "                   F.when(F.col(\"primary_energy_consumption\") > 0,\n",
    "                         F.col(\"renewables_consumption\") / F.col(\"primary_energy_consumption\"))\n",
    "                   .otherwise(0)) \\\n",
    "        .withColumn(\"decade\", (F.floor(F.col(\"year\") / 10) * 10).cast(IntegerType())) \\\n",
    "        .withColumn(\"processing_timestamp\", F.current_timestamp())\n",
    "    \n",
    "    print(f\"âœ“ Features creados: renewable_ratio, decade, processing_timestamp\")\n",
    "    \n",
    "    # LOAD\n",
    "    print(\"\\n[4/5] LOAD - Guardando en Delta Lake...\")\n",
    "    writer = df_transformed.write \\\n",
    "        .format(\"delta\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .option(\"overwriteSchema\", \"true\")\n",
    "    \n",
    "    if partition_cols:\n",
    "        writer = writer.partitionBy(*partition_cols)\n",
    "        print(f\"âœ“ Particionado por: {', '.join(partition_cols)}\")\n",
    "    \n",
    "    writer.save(target_path)\n",
    "    \n",
    "    # OPTIMIZE\n",
    "    print(\"\\n[5/5] OPTIMIZE - Optimizando Delta Table...\")\n",
    "    spark.sql(f\"OPTIMIZE delta.`{target_path}` ZORDER BY (country, year)\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"PIPELINE ETL COMPLETADO CON Ã‰XITO\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"ðŸ“Š Registros procesados: {clean_count:,}\")\n",
    "    print(f\"ðŸ“ UbicaciÃ³n: {target_path}\")\n",
    "    print(f\"âš¡ Optimizado con Z-ORDER\")\n",
    "    \n",
    "    return clean_count\n",
    "\n",
    "print(\"âœ“ FunciÃ³n ETL pipeline definida\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aaa318b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejecutar pipeline completo\n",
    "records_processed = etl_pipeline(\n",
    "    source_path=\"/workspaces/AzureDatabricksWorkshop/FundamentosArquitecturaAzureDatabricks/owid-energy-data.csv\",\n",
    "    target_path=\"/tmp/delta/energy_final\",\n",
    "    partition_cols=[\"decade\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780f683c",
   "metadata": {},
   "source": [
    "## 14. Monitoreo y Reporte de Salud\n",
    "\n",
    "Generamos mÃ©tricas y reportes de la tabla Delta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126de00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def monitor_delta_table(delta_path):\n",
    "    \"\"\"\n",
    "    Genera reporte de salud de tabla Delta\n",
    "    \"\"\"\n",
    "    delta_table = DeltaTable.forPath(spark, delta_path)\n",
    "    df = spark.read.format(\"delta\").load(delta_path)\n",
    "    detail = spark.sql(f\"DESCRIBE DETAIL delta.`{delta_path}`\").first()\n",
    "    \n",
    "    report = {\n",
    "        \"total_records\": df.count(),\n",
    "        \"total_columns\": len(df.columns),\n",
    "        \"table_size_mb\": round(detail[\"sizeInBytes\"] / (1024*1024), 2),\n",
    "        \"number_of_files\": detail[\"numFiles\"],\n",
    "        \"latest_version\": delta_table.history().select(\"version\").first()[0]\n",
    "    }\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"REPORTE DE SALUD - DELTA TABLE\")\n",
    "    print(\"=\"*60)\n",
    "    for key, value in report.items():\n",
    "        print(f\"{key.replace('_', ' ').title()}: {value:,}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    return report\n",
    "\n",
    "# Generar reporte\n",
    "health_report = monitor_delta_table(\"/tmp/delta/energy_final\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef88c5e8",
   "metadata": {},
   "source": [
    "## 15. Consultas SQL sobre Delta Lake\n",
    "\n",
    "Demostramos queries SQL sobre las tablas Delta creadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61db4bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear vista temporal para queries SQL\n",
    "df_final = spark.read.format(\"delta\").load(\"/tmp/delta/energy_final\")\n",
    "df_final.createOrReplaceTempView(\"energy_analysis\")\n",
    "\n",
    "print(\"âœ“ Vista temporal 'energy_analysis' creada\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebce339a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "-- Top 10 paÃ­ses con mayor ratio de renovables (2020)\n",
    "SELECT \n",
    "    country,\n",
    "    year,\n",
    "    ROUND(renewable_ratio * 100, 2) as renewable_percentage,\n",
    "    primary_energy_consumption\n",
    "FROM energy_analysis\n",
    "WHERE year = 2020\n",
    "  AND primary_energy_consumption > 100\n",
    "ORDER BY renewable_ratio DESC\n",
    "LIMIT 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e00c87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "-- EvoluciÃ³n promedio de renovables por dÃ©cada\n",
    "SELECT \n",
    "    decade,\n",
    "    ROUND(AVG(renewable_ratio) * 100, 2) as avg_renewable_pct,\n",
    "    COUNT(*) as country_count\n",
    "FROM energy_analysis\n",
    "WHERE decade >= 1980\n",
    "GROUP BY decade\n",
    "ORDER BY decade"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c009e01",
   "metadata": {},
   "source": [
    "## 16. ConclusiÃ³n y Resumen\n",
    "\n",
    "Â¡Felicitaciones! Has completado el Lab 2 de PreparaciÃ³n y Procesamiento de Datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9e83c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resumen de lo aprendido\n",
    "summary = {\n",
    "    \"Habilidades Adquiridas\": [\n",
    "        \"âœ… ExploraciÃ³n de datos con mÃºltiples lenguajes\",\n",
    "        \"âœ… Ingesta eficiente con Auto Loader\",\n",
    "        \"âœ… Limpieza y transformaciÃ³n con PySpark\",\n",
    "        \"âœ… ImplementaciÃ³n de Delta Lake con ACID\",\n",
    "        \"âœ… Versionado y Time Travel\",\n",
    "        \"âœ… OptimizaciÃ³n con OPTIMIZE y Z-ORDER\",\n",
    "        \"âœ… Schema evolution y escalabilidad\",\n",
    "        \"âœ… Pipelines ETL reproducibles\"\n",
    "    ],\n",
    "    \"Archivos Creados\": [\n",
    "        \"/tmp/delta/energy_processed\",\n",
    "        \"/tmp/delta/energy_partitioned\",\n",
    "        \"/tmp/delta/energy_final\"\n",
    "    ],\n",
    "    \"PrÃ³ximos Pasos\": [\n",
    "        \"Lab 3: Feature Engineering y ExploraciÃ³n de Datos\",\n",
    "        \"Lab 4: Entrenamiento de Modelos con MLflow\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"LABORATORIO COMPLETADO\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for section, items in summary.items():\n",
    "    print(f\"\\n{section}:\")\n",
    "    for item in items:\n",
    "        print(f\"  {item}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Â¡EXCELENTE TRABAJO!\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
