{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aed5f143",
   "metadata": {},
   "source": [
    "## Parte 1: Batch Scoring - Predicciones por Lotes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea837c4",
   "metadata": {},
   "source": [
    "### 1.1 Cargar Modelo desde MLflow Registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0e0f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.pyfunc\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import struct, col\n",
    "import pandas as pd\n",
    "\n",
    "# Configurar MLflow\n",
    "mlflow.set_registry_uri(\"databricks\")\n",
    "\n",
    "# Nombre del modelo y versi√≥n (ajustar seg√∫n Lab 4)\n",
    "model_name = \"diabetes_predictor\"  # Cambiar seg√∫n tu modelo\n",
    "model_version = 1  # O \"Production\" para usar el stage\n",
    "\n",
    "# Cargar modelo\n",
    "model_uri = f\"models:/{model_name}/{model_version}\"\n",
    "print(f\"Cargando modelo desde: {model_uri}\")\n",
    "\n",
    "loaded_model = mlflow.pyfunc.load_model(model_uri)\n",
    "print(f\"‚úì Modelo cargado exitosamente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc06e0a",
   "metadata": {},
   "source": [
    "### 1.2 Preparar Datos de Entrada para Batch Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81341a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear datos de ejemplo para predicciones\n",
    "sample_data = pd.DataFrame({\n",
    "    'age': [45, 52, 38, 61, 29, 47, 55, 42, 36, 58],\n",
    "    'bmi': [27.3, 32.1, 24.5, 28.9, 22.4, 29.5, 31.8, 26.2, 23.7, 30.4],\n",
    "    'blood_pressure': [120, 135, 110, 140, 115, 125, 138, 118, 112, 142],\n",
    "    'glucose': [95, 160, 88, 175, 82, 105, 155, 92, 85, 168]\n",
    "})\n",
    "\n",
    "display(sample_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1e6cf2",
   "metadata": {},
   "source": [
    "### 1.3 Realizar Predicciones B√°sicas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db70b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicciones con pandas DataFrame\n",
    "predictions = loaded_model.predict(sample_data)\n",
    "\n",
    "# Agregar predicciones al DataFrame\n",
    "result_df = sample_data.copy()\n",
    "result_df['prediction'] = predictions\n",
    "result_df['prediction_timestamp'] = pd.Timestamp.now()\n",
    "\n",
    "print(f\"Total de predicciones: {len(result_df)}\")\n",
    "display(result_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f070c88",
   "metadata": {},
   "source": [
    "### 1.4 Batch Scoring a Gran Escala con Spark UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20b66ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import pandas_udf\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "# Crear Spark DataFrame\n",
    "spark_df = spark.createDataFrame(sample_data)\n",
    "\n",
    "# Definir UDF para predicciones distribuidas\n",
    "@pandas_udf(DoubleType())\n",
    "def predict_udf(*cols):\n",
    "    # Reconstruir DataFrame desde columnas\n",
    "    input_df = pd.concat(cols, axis=1)\n",
    "    input_df.columns = sample_data.columns\n",
    "    # Hacer predicci√≥n\n",
    "    return pd.Series(loaded_model.predict(input_df))\n",
    "\n",
    "# Aplicar predicciones\n",
    "predictions_df = spark_df.withColumn(\n",
    "    \"prediction\",\n",
    "    predict_udf(*[col(c) for c in sample_data.columns])\n",
    ")\n",
    "\n",
    "display(predictions_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0ede9a",
   "metadata": {},
   "source": [
    "### 1.5 Guardar Resultados en Delta Lake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74df0d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir ruta de salida\n",
    "output_path = \"/tmp/predictions/diabetes/batch_scoring\"\n",
    "\n",
    "# Guardar en Delta Lake\n",
    "predictions_df.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"append\") \\\n",
    "    .option(\"mergeSchema\", \"true\") \\\n",
    "    .save(output_path)\n",
    "\n",
    "print(f\"‚úì Predicciones guardadas en: {output_path}\")\n",
    "\n",
    "# Verificar datos guardados\n",
    "saved_df = spark.read.format(\"delta\").load(output_path)\n",
    "print(f\"Total de registros guardados: {saved_df.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885837d0",
   "metadata": {},
   "source": [
    "### 1.6 Registrar M√©tricas de Batch Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d80da1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# Contar predicciones generadas\n",
    "prediction_count = predictions_df.count()\n",
    "\n",
    "# Registrar m√©tricas en MLflow\n",
    "with mlflow.start_run(run_name=\"batch_scoring_metrics\"):\n",
    "    mlflow.log_metric(\"predictions_count\", prediction_count)\n",
    "    mlflow.log_metric(\"execution_timestamp\", datetime.now().timestamp())\n",
    "    mlflow.log_param(\"model_name\", model_name)\n",
    "    mlflow.log_param(\"model_version\", model_version)\n",
    "    mlflow.log_param(\"output_path\", output_path)\n",
    "    \n",
    "print(f\"‚úì M√©tricas registradas en MLflow\")\n",
    "print(f\"  - Predicciones: {prediction_count}\")\n",
    "print(f\"  - Timestamp: {datetime.now()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b869fb",
   "metadata": {},
   "source": [
    "## Parte 2: Despliegue de Endpoints en Tiempo Real"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc736e6",
   "metadata": {},
   "source": [
    "### 2.1 Crear Endpoint REST con MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048d9440",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflow.deployments import get_deploy_client\n",
    "\n",
    "# Configurar cliente de despliegue\n",
    "client = get_deploy_client(\"databricks\")\n",
    "\n",
    "# Configuraci√≥n del endpoint\n",
    "endpoint_name = \"diabetes-predictor-endpoint\"\n",
    "\n",
    "endpoint_config = {\n",
    "    \"served_models\": [{\n",
    "        \"model_name\": model_name,\n",
    "        \"model_version\": str(model_version),\n",
    "        \"workload_size\": \"Small\",  # Small, Medium, Large\n",
    "        \"scale_to_zero_enabled\": True  # Escalar a 0 cuando no hay tr√°fico\n",
    "    }]\n",
    "}\n",
    "\n",
    "# Crear endpoint\n",
    "try:\n",
    "    endpoint = client.create_endpoint(\n",
    "        name=endpoint_name,\n",
    "        config=endpoint_config\n",
    "    )\n",
    "    print(f\"‚úì Endpoint creado: {endpoint_name}\")\n",
    "except Exception as e:\n",
    "    print(f\"Endpoint ya existe o error: {e}\")\n",
    "    print(\"Continuando con el endpoint existente...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b4d8295",
   "metadata": {},
   "source": [
    "### 2.2 Verificar Estado del Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1244934",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Esperar a que el endpoint est√© listo\n",
    "print(\"Verificando estado del endpoint...\")\n",
    "max_wait = 300  # 5 minutos\n",
    "wait_interval = 10\n",
    "elapsed = 0\n",
    "\n",
    "while elapsed < max_wait:\n",
    "    try:\n",
    "        endpoint_details = client.get_endpoint(endpoint_name)\n",
    "        state = endpoint_details.get('state', {})\n",
    "        \n",
    "        if state.get('ready') == 'READY':\n",
    "            print(f\"\\n‚úì Endpoint listo para recibir tr√°fico\")\n",
    "            print(f\"Estado: {state}\")\n",
    "            break\n",
    "        else:\n",
    "            print(f\"Esperando... ({elapsed}s/{max_wait}s) - Estado: {state.get('ready', 'UNKNOWN')}\")\n",
    "            time.sleep(wait_interval)\n",
    "            elapsed += wait_interval\n",
    "    except Exception as e:\n",
    "        print(f\"Error al verificar estado: {e}\")\n",
    "        break\n",
    "\n",
    "if elapsed >= max_wait:\n",
    "    print(f\"‚ö†Ô∏è Tiempo de espera agotado. El endpoint puede tardar m√°s en estar listo.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb942bb",
   "metadata": {},
   "source": [
    "### 2.3 Consumir Endpoint REST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6e5613",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "# Configuraci√≥n\n",
    "workspace_url = spark.conf.get(\"spark.databricks.workspaceUrl\")\n",
    "token = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiToken().get()\n",
    "\n",
    "endpoint_url = f\"https://{workspace_url}/serving-endpoints/{endpoint_name}/invocations\"\n",
    "\n",
    "# Datos de prueba\n",
    "test_data = {\n",
    "    \"dataframe_records\": [\n",
    "        {\"age\": 45, \"bmi\": 27.3, \"blood_pressure\": 120, \"glucose\": 95},\n",
    "        {\"age\": 52, \"bmi\": 32.1, \"blood_pressure\": 135, \"glucose\": 160}\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Headers\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {token}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "# Realizar petici√≥n\n",
    "start_time = time.time()\n",
    "response = requests.post(\n",
    "    endpoint_url,\n",
    "    headers=headers,\n",
    "    json=test_data\n",
    ")\n",
    "latency = (time.time() - start_time) * 1000  # ms\n",
    "\n",
    "# Procesar respuesta\n",
    "if response.status_code == 200:\n",
    "    predictions = response.json()\n",
    "    print(\"‚úì Predicciones recibidas:\")\n",
    "    print(json.dumps(predictions, indent=2))\n",
    "    print(f\"\\nLatencia: {latency:.2f} ms\")\n",
    "else:\n",
    "    print(f\"‚úó Error: {response.status_code}\")\n",
    "    print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66688af",
   "metadata": {},
   "source": [
    "## Parte 3: Monitoreo y Logging de Predicciones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f9f2b3",
   "metadata": {},
   "source": [
    "### 3.1 Implementar Logging de Predicciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d86703a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "from datetime import datetime\n",
    "\n",
    "class PredictionLogger:\n",
    "    \"\"\"Logger para predicciones con metadata completa\"\"\"\n",
    "    \n",
    "    def __init__(self, log_table_path, model_name, model_version):\n",
    "        self.log_table_path = log_table_path\n",
    "        self.model_name = model_name\n",
    "        self.model_version = model_version\n",
    "    \n",
    "    def log_prediction(self, input_data, prediction, latency_ms, request_id=None):\n",
    "        \"\"\"Registra predicci√≥n con metadata\"\"\"\n",
    "        if request_id is None:\n",
    "            request_id = str(uuid.uuid4())\n",
    "        \n",
    "        # Preparar log entry\n",
    "        log_entry = {\n",
    "            'request_id': request_id,\n",
    "            'timestamp': datetime.now(),\n",
    "            'model_name': self.model_name,\n",
    "            'model_version': str(self.model_version),\n",
    "            'prediction': float(prediction),\n",
    "            'latency_ms': latency_ms,\n",
    "            **input_data\n",
    "        }\n",
    "        \n",
    "        # Guardar en Delta Lake\n",
    "        log_df = spark.createDataFrame([log_entry])\n",
    "        log_df.write.format(\"delta\").mode(\"append\").save(self.log_table_path)\n",
    "        \n",
    "        return request_id\n",
    "\n",
    "# Crear logger\n",
    "log_path = \"/tmp/ml-monitoring/prediction-logs\"\n",
    "logger = PredictionLogger(log_path, model_name, model_version)\n",
    "\n",
    "print(f\"‚úì Logger configurado en: {log_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a646bd72",
   "metadata": {},
   "source": [
    "### 3.2 Generar Tr√°fico de Prueba con Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b722a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Generar tr√°fico sint√©tico\n",
    "print(\"Generando tr√°fico de prueba...\\n\")\n",
    "\n",
    "num_requests = 20\n",
    "for i in range(num_requests):\n",
    "    # Generar datos de entrada\n",
    "    test_input = {\n",
    "        'age': int(np.random.uniform(25, 75)),\n",
    "        'bmi': round(np.random.uniform(18, 40), 1),\n",
    "        'blood_pressure': int(np.random.uniform(100, 160)),\n",
    "        'glucose': int(np.random.uniform(70, 200))\n",
    "    }\n",
    "    \n",
    "    # Realizar predicci√≥n\n",
    "    start = time.time()\n",
    "    pred_input = pd.DataFrame([test_input])\n",
    "    prediction = loaded_model.predict(pred_input)[0]\n",
    "    latency = (time.time() - start) * 1000\n",
    "    \n",
    "    # Registrar en log\n",
    "    req_id = logger.log_prediction(test_input, prediction, latency)\n",
    "    \n",
    "    if i % 5 == 0:\n",
    "        print(f\"Request {i+1}/{num_requests}: Prediction={prediction:.3f}, Latency={latency:.1f}ms\")\n",
    "\n",
    "print(f\"\\n‚úì Tr√°fico de prueba completado: {num_requests} predicciones\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e2aa16",
   "metadata": {},
   "source": [
    "### 3.3 Analizar Logs de Predicciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1da5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leer logs\n",
    "logs_df = spark.read.format(\"delta\").load(log_path)\n",
    "\n",
    "print(f\"üìä Total de predicciones registradas: {logs_df.count()}\")\n",
    "\n",
    "# Convertir a pandas para an√°lisis\n",
    "logs_pd = logs_df.toPandas()\n",
    "\n",
    "# Estad√≠sticas de latencia\n",
    "print(f\"\\nüìà Estad√≠sticas de Latencia:\")\n",
    "print(f\"  Media: {logs_pd['latency_ms'].mean():.2f} ms\")\n",
    "print(f\"  Mediana (P50): {logs_pd['latency_ms'].quantile(0.5):.2f} ms\")\n",
    "print(f\"  P95: {logs_pd['latency_ms'].quantile(0.95):.2f} ms\")\n",
    "print(f\"  P99: {logs_pd['latency_ms'].quantile(0.99):.2f} ms\")\n",
    "print(f\"  Min: {logs_pd['latency_ms'].min():.2f} ms\")\n",
    "print(f\"  Max: {logs_pd['latency_ms'].max():.2f} ms\")\n",
    "\n",
    "# Estad√≠sticas de predicciones\n",
    "print(f\"\\nüéØ Estad√≠sticas de Predicciones:\")\n",
    "print(f\"  Media: {logs_pd['prediction'].mean():.3f}\")\n",
    "print(f\"  Std Dev: {logs_pd['prediction'].std():.3f}\")\n",
    "print(f\"  Min: {logs_pd['prediction'].min():.3f}\")\n",
    "print(f\"  Max: {logs_pd['prediction'].max():.3f}\")\n",
    "\n",
    "# Mostrar muestra de logs\n",
    "display(logs_df.orderBy(col(\"timestamp\").desc()).limit(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9d9eb9",
   "metadata": {},
   "source": [
    "### 3.4 Visualizar M√©tricas de Monitoreo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28e49c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Configurar estilo\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# Crear dashboard de monitoreo\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# 1. Distribuci√≥n de latencia\n",
    "axes[0, 0].hist(logs_pd['latency_ms'], bins=20, edgecolor='black', alpha=0.7)\n",
    "axes[0, 0].axvline(logs_pd['latency_ms'].mean(), color='red', linestyle='--', label='Media')\n",
    "axes[0, 0].axvline(logs_pd['latency_ms'].quantile(0.95), color='orange', linestyle='--', label='P95')\n",
    "axes[0, 0].set_title('Distribuci√≥n de Latencia', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Latencia (ms)')\n",
    "axes[0, 0].set_ylabel('Frecuencia')\n",
    "axes[0, 0].legend()\n",
    "\n",
    "# 2. Distribuci√≥n de predicciones\n",
    "axes[0, 1].hist(logs_pd['prediction'], bins=25, edgecolor='black', alpha=0.7, color='green')\n",
    "axes[0, 1].set_title('Distribuci√≥n de Predicciones', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Valor Predicho')\n",
    "axes[0, 1].set_ylabel('Frecuencia')\n",
    "\n",
    "# 3. Predicciones vs Edad\n",
    "axes[1, 0].scatter(logs_pd['age'], logs_pd['prediction'], alpha=0.6, s=50)\n",
    "axes[1, 0].set_title('Predicciones vs Edad', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Edad')\n",
    "axes[1, 0].set_ylabel('Predicci√≥n')\n",
    "\n",
    "# 4. Predicciones vs BMI\n",
    "axes[1, 1].scatter(logs_pd['bmi'], logs_pd['prediction'], alpha=0.6, s=50, color='purple')\n",
    "axes[1, 1].set_title('Predicciones vs BMI', fontsize=12, fontweight='bold')\n",
    "axes[1, 1].set_xlabel('BMI')\n",
    "axes[1, 1].set_ylabel('Predicci√≥n')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('/tmp/monitoring_dashboard.png', dpi=100, bbox_inches='tight')\n",
    "display(plt.gcf())\n",
    "\n",
    "# Registrar dashboard en MLflow\n",
    "with mlflow.start_run(run_name=\"monitoring_dashboard\"):\n",
    "    mlflow.log_artifact('/tmp/monitoring_dashboard.png')\n",
    "    mlflow.log_metric(\"total_predictions\", len(logs_pd))\n",
    "    mlflow.log_metric(\"avg_latency_ms\", logs_pd['latency_ms'].mean())\n",
    "    mlflow.log_metric(\"p95_latency_ms\", logs_pd['latency_ms'].quantile(0.95))\n",
    "\n",
    "print(\"\\n‚úì Dashboard generado y registrado en MLflow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44abcfa2",
   "metadata": {},
   "source": [
    "## Parte 4: Detecci√≥n de Data Drift"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b050662f",
   "metadata": {},
   "source": [
    "### 4.1 Implementar Detecci√≥n de Drift con KS Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25918502",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "def calculate_drift(reference_data, current_data, features, threshold=0.05):\n",
    "    \"\"\"\n",
    "    Detecta drift usando Kolmogorov-Smirnov test\n",
    "    \n",
    "    Args:\n",
    "        reference_data: DataFrame con datos de referencia (training)\n",
    "        current_data: DataFrame con datos actuales (producci√≥n)\n",
    "        features: Lista de features a analizar\n",
    "        threshold: P-value threshold para detectar drift\n",
    "    \n",
    "    Returns:\n",
    "        Dict con resultados de drift por feature\n",
    "    \"\"\"\n",
    "    drift_results = {}\n",
    "    \n",
    "    for feature in features:\n",
    "        # KS test\n",
    "        statistic, p_value = stats.ks_2samp(\n",
    "            reference_data[feature],\n",
    "            current_data[feature]\n",
    "        )\n",
    "        \n",
    "        # Drift detectado si p-value < threshold\n",
    "        drift_results[feature] = {\n",
    "            'statistic': statistic,\n",
    "            'p_value': p_value,\n",
    "            'drift_detected': p_value < threshold,\n",
    "            'severity': 'HIGH' if p_value < 0.01 else 'MEDIUM' if p_value < threshold else 'LOW'\n",
    "        }\n",
    "    \n",
    "    return drift_results\n",
    "\n",
    "print(\"‚úì Funci√≥n de detecci√≥n de drift implementada\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60517b2d",
   "metadata": {},
   "source": [
    "### 4.2 Comparar Datos de Training vs Producci√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de94c0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datos de referencia (simulados - en producci√≥n vendr√≠an del training set)\n",
    "reference_data = pd.DataFrame({\n",
    "    'age': np.random.normal(50, 10, 1000),\n",
    "    'bmi': np.random.normal(28, 5, 1000),\n",
    "    'blood_pressure': np.random.normal(130, 15, 1000),\n",
    "    'glucose': np.random.normal(120, 30, 1000)\n",
    "})\n",
    "\n",
    "# Datos de producci√≥n\n",
    "production_data = logs_pd[['age', 'bmi', 'blood_pressure', 'glucose']]\n",
    "\n",
    "# Detectar drift\n",
    "features = ['age', 'bmi', 'blood_pressure', 'glucose']\n",
    "drift_results = calculate_drift(reference_data, production_data, features, threshold=0.05)\n",
    "\n",
    "# Mostrar resultados\n",
    "print(\"üîç An√°lisis de Data Drift:\\n\")\n",
    "print(f\"{'Feature':<20} {'P-Value':<12} {'Statistic':<12} {'Estado':<15} {'Severidad'}\")\n",
    "print(\"-\" * 75)\n",
    "\n",
    "for feature, result in drift_results.items():\n",
    "    status = \"‚ö†Ô∏è DRIFT\" if result['drift_detected'] else \"‚úì Sin Drift\"\n",
    "    print(f\"{feature:<20} {result['p_value']:<12.4f} {result['statistic']:<12.4f} {status:<15} {result['severity']}\")\n",
    "\n",
    "# Contar features con drift\n",
    "drift_count = sum(1 for r in drift_results.values() if r['drift_detected'])\n",
    "print(f\"\\nResumen: {drift_count}/{len(features)} features con drift detectado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b397b7",
   "metadata": {},
   "source": [
    "### 4.3 Visualizar Drift por Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8103aba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar comparaci√≥n de distribuciones\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, feature in enumerate(features):\n",
    "    # Histogramas superpuestos\n",
    "    axes[idx].hist(reference_data[feature], bins=30, alpha=0.5, label='Training (Reference)', \n",
    "                   edgecolor='black', density=True)\n",
    "    axes[idx].hist(production_data[feature], bins=30, alpha=0.5, label='Production (Current)', \n",
    "                   edgecolor='black', density=True, color='orange')\n",
    "    \n",
    "    # T√≠tulo con informaci√≥n de drift\n",
    "    drift_info = drift_results[feature]\n",
    "    status = \"‚ö†Ô∏è DRIFT DETECTADO\" if drift_info['drift_detected'] else \"‚úì Sin Drift\"\n",
    "    axes[idx].set_title(f'{feature}\\n{status} (p={drift_info[\"p_value\"]:.4f})', \n",
    "                       fontsize=11, fontweight='bold')\n",
    "    axes[idx].set_xlabel(feature)\n",
    "    axes[idx].set_ylabel('Densidad')\n",
    "    axes[idx].legend()\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('/tmp/drift_analysis.png', dpi=100, bbox_inches='tight')\n",
    "display(plt.gcf())\n",
    "\n",
    "print(\"‚úì Visualizaci√≥n de drift generada\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5512ccb1",
   "metadata": {},
   "source": [
    "## Parte 5: Sistema de Alertas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eadd77c1",
   "metadata": {},
   "source": [
    "### 5.1 Configurar Sistema de Alertas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b9af8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlertSystem:\n",
    "    \"\"\"Sistema de alertas para monitoreo de modelos\"\"\"\n",
    "    \n",
    "    def __init__(self, thresholds):\n",
    "        self.thresholds = thresholds\n",
    "        self.alerts = []\n",
    "    \n",
    "    def check_latency(self, metrics):\n",
    "        \"\"\"Verificar latencia\"\"\"\n",
    "        if metrics.get('p95_latency_ms', 0) > self.thresholds['max_latency_p95_ms']:\n",
    "            self.alerts.append({\n",
    "                'type': 'HIGH_LATENCY',\n",
    "                'severity': 'WARNING',\n",
    "                'message': f\"Latencia P95 ({metrics['p95_latency_ms']:.1f}ms) excede threshold ({self.thresholds['max_latency_p95_ms']}ms)\",\n",
    "                'value': metrics['p95_latency_ms']\n",
    "            })\n",
    "    \n",
    "    def check_drift(self, drift_results):\n",
    "        \"\"\"Verificar data drift\"\"\"\n",
    "        drift_features = [f for f, r in drift_results.items() if r['drift_detected']]\n",
    "        if drift_features:\n",
    "            self.alerts.append({\n",
    "                'type': 'DATA_DRIFT',\n",
    "                'severity': 'WARNING',\n",
    "                'message': f'Drift detectado en features: {\", \".join(drift_features)}',\n",
    "                'features': drift_features\n",
    "            })\n",
    "    \n",
    "    def check_prediction_distribution(self, predictions, expected_mean, tolerance=0.2):\n",
    "        \"\"\"Verificar distribuci√≥n de predicciones\"\"\"\n",
    "        current_mean = predictions.mean()\n",
    "        deviation = abs(current_mean - expected_mean) / expected_mean\n",
    "        \n",
    "        if deviation > tolerance:\n",
    "            self.alerts.append({\n",
    "                'type': 'PREDICTION_DISTRIBUTION_SHIFT',\n",
    "                'severity': 'CRITICAL',\n",
    "                'message': f'Media de predicciones ({current_mean:.3f}) difiere significativamente de la esperada ({expected_mean:.3f})',\n",
    "                'deviation': deviation\n",
    "            })\n",
    "    \n",
    "    def send_alerts(self):\n",
    "        \"\"\"Enviar alertas (simulado)\"\"\"\n",
    "        if not self.alerts:\n",
    "            print(\"‚úì No hay alertas - Sistema operando normalmente\")\n",
    "            return\n",
    "        \n",
    "        print(f\"\\nüö® {len(self.alerts)} ALERTA(S) DETECTADA(S):\\n\")\n",
    "        for i, alert in enumerate(self.alerts, 1):\n",
    "            print(f\"{i}. [{alert['severity']}] {alert['type']}\")\n",
    "            print(f\"   {alert['message']}\\n\")\n",
    "        \n",
    "        # En producci√≥n, aqu√≠ se integrar√≠a con:\n",
    "        # - Email (SendGrid, SMTP)\n",
    "        # - Slack webhook\n",
    "        # - Microsoft Teams webhook\n",
    "        # - PagerDuty\n",
    "        # - Azure Monitor\n",
    "    \n",
    "    def get_alert_summary(self):\n",
    "        \"\"\"Resumen de alertas\"\"\"\n",
    "        return {\n",
    "            'total': len(self.alerts),\n",
    "            'critical': sum(1 for a in self.alerts if a['severity'] == 'CRITICAL'),\n",
    "            'warning': sum(1 for a in self.alerts if a['severity'] == 'WARNING')\n",
    "        }\n",
    "\n",
    "print(\"‚úì Sistema de alertas implementado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d43b454",
   "metadata": {},
   "source": [
    "### 5.2 Ejecutar Verificaciones y Generar Alertas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038ea35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurar thresholds\n",
    "thresholds = {\n",
    "    'max_latency_p95_ms': 200,\n",
    "    'min_accuracy': 0.85,\n",
    "    'max_drift_pvalue': 0.05\n",
    "}\n",
    "\n",
    "# Crear sistema de alertas\n",
    "alert_system = AlertSystem(thresholds)\n",
    "\n",
    "# Ejecutar verificaciones\n",
    "metrics = {\n",
    "    'p95_latency_ms': logs_pd['latency_ms'].quantile(0.95),\n",
    "    'avg_latency_ms': logs_pd['latency_ms'].mean()\n",
    "}\n",
    "\n",
    "alert_system.check_latency(metrics)\n",
    "alert_system.check_drift(drift_results)\n",
    "alert_system.check_prediction_distribution(\n",
    "    logs_pd['prediction'], \n",
    "    expected_mean=0.5,  # Ajustar seg√∫n modelo\n",
    "    tolerance=0.3\n",
    ")\n",
    "\n",
    "# Enviar alertas\n",
    "alert_system.send_alerts()\n",
    "\n",
    "# Mostrar resumen\n",
    "summary = alert_system.get_alert_summary()\n",
    "print(f\"\\nüìä Resumen de Alertas:\")\n",
    "print(f\"  Total: {summary['total']}\")\n",
    "print(f\"  Cr√≠ticas: {summary['critical']}\")\n",
    "print(f\"  Advertencias: {summary['warning']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735e9b8c",
   "metadata": {},
   "source": [
    "## Parte 6: Buenas Pr√°cticas - Model Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09659f96",
   "metadata": {},
   "source": [
    "### 6.1 Aplicar Tags al Modelo en Producci√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5284309",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "client = MlflowClient()\n",
    "\n",
    "# Tags recomendados para producci√≥n\n",
    "production_tags = {\n",
    "    # Metadata t√©cnica\n",
    "    \"model_type\": \"regression\",\n",
    "    \"framework\": \"sklearn\",\n",
    "    \n",
    "    # Informaci√≥n del dataset\n",
    "    \"training_date\": datetime.now().strftime(\"%Y-%m-%d\"),\n",
    "    \"training_samples\": \"1000\",\n",
    "    \n",
    "    # Deployment info\n",
    "    \"deployed_by\": \"data-science-team\",\n",
    "    \"deployment_date\": datetime.now().strftime(\"%Y-%m-%d\"),\n",
    "    \"environment\": \"production\",\n",
    "    \n",
    "    # Monitoring\n",
    "    \"monitoring_enabled\": \"true\",\n",
    "    \"alert_threshold_latency_p95\": \"200\",\n",
    "    \n",
    "    # Business context\n",
    "    \"use_case\": \"diabetes-prediction\",\n",
    "    \"business_owner\": \"healthcare-analytics\"\n",
    "}\n",
    "\n",
    "# Aplicar tags\n",
    "try:\n",
    "    for key, value in production_tags.items():\n",
    "        client.set_model_version_tag(model_name, model_version, key, value)\n",
    "    print(f\"‚úì {len(production_tags)} tags aplicados al modelo {model_name} v{model_version}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error al aplicar tags: {e}\")\n",
    "\n",
    "# Verificar tags\n",
    "try:\n",
    "    model_version_info = client.get_model_version(model_name, model_version)\n",
    "    print(f\"\\nTags actuales:\")\n",
    "    for key, value in model_version_info.tags.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "except Exception as e:\n",
    "    print(f\"No se pudieron verificar tags: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d585ec",
   "metadata": {},
   "source": [
    "## Parte 7: Resumen y Reporte Final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951fcaf5",
   "metadata": {},
   "source": [
    "### 7.1 Generar Reporte de Monitoreo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57ddbb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generar reporte completo\n",
    "report = f\"\"\"\n",
    "{'='*80}\n",
    "REPORTE DE MONITOREO - MODELO EN PRODUCCI√ìN\n",
    "{'='*80}\n",
    "\n",
    "üìã INFORMACI√ìN DEL MODELO\n",
    "  Nombre: {model_name}\n",
    "  Versi√≥n: {model_version}\n",
    "  Endpoint: {endpoint_name}\n",
    "  Fecha de reporte: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "\n",
    "üìä M√âTRICAS DE SERVICIO\n",
    "  Total de predicciones: {len(logs_pd)}\n",
    "  Latencia media: {logs_pd['latency_ms'].mean():.2f} ms\n",
    "  Latencia P95: {logs_pd['latency_ms'].quantile(0.95):.2f} ms\n",
    "  Latencia P99: {logs_pd['latency_ms'].quantile(0.99):.2f} ms\n",
    "\n",
    "üéØ ESTAD√çSTICAS DE PREDICCIONES\n",
    "  Media: {logs_pd['prediction'].mean():.4f}\n",
    "  Desviaci√≥n est√°ndar: {logs_pd['prediction'].std():.4f}\n",
    "  Rango: [{logs_pd['prediction'].min():.4f}, {logs_pd['prediction'].max():.4f}]\n",
    "\n",
    "üîç DETECCI√ìN DE DRIFT\n",
    "  Features analizados: {len(features)}\n",
    "  Features con drift: {sum(1 for r in drift_results.values() if r['drift_detected'])}\n",
    "\"\"\"\n",
    "\n",
    "# Agregar detalles de drift\n",
    "for feature, result in drift_results.items():\n",
    "    if result['drift_detected']:\n",
    "        report += f\"    ‚ö†Ô∏è {feature}: p-value={result['p_value']:.4f}\\n\"\n",
    "\n",
    "report += f\"\"\"\n",
    "üö® ALERTAS\n",
    "  Total de alertas: {summary['total']}\n",
    "  Cr√≠ticas: {summary['critical']}\n",
    "  Advertencias: {summary['warning']}\n",
    "\n",
    "‚úÖ RECOMENDACIONES\n",
    "\"\"\"\n",
    "\n",
    "# Generar recomendaciones din√°micas\n",
    "if summary['total'] == 0:\n",
    "    report += \"  - Sistema operando dentro de par√°metros normales\\n\"\n",
    "    report += \"  - Continuar con monitoreo regular\\n\"\n",
    "else:\n",
    "    if summary['critical'] > 0:\n",
    "        report += \"  - ‚ö†Ô∏è ACCI√ìN INMEDIATA REQUERIDA: Alertas cr√≠ticas detectadas\\n\"\n",
    "    if any(r['drift_detected'] for r in drift_results.values()):\n",
    "        report += \"  - Considerar reentrenamiento del modelo debido a drift\\n\"\n",
    "    if logs_pd['latency_ms'].quantile(0.95) > thresholds['max_latency_p95_ms']:\n",
    "        report += \"  - Optimizar latencia o escalar recursos del endpoint\\n\"\n",
    "\n",
    "report += f\"\"\"\n",
    "{'='*80}\n",
    "\"\"\"\n",
    "\n",
    "print(report)\n",
    "\n",
    "# Guardar reporte\n",
    "report_path = \"/tmp/monitoring_report.txt\"\n",
    "with open(report_path, 'w') as f:\n",
    "    f.write(report)\n",
    "\n",
    "# Registrar en MLflow\n",
    "with mlflow.start_run(run_name=\"monitoring_report\"):\n",
    "    mlflow.log_artifact(report_path)\n",
    "    mlflow.log_artifact('/tmp/monitoring_dashboard.png')\n",
    "    mlflow.log_artifact('/tmp/drift_analysis.png')\n",
    "    \n",
    "    # M√©tricas principales\n",
    "    mlflow.log_metrics({\n",
    "        \"total_predictions\": len(logs_pd),\n",
    "        \"avg_latency_ms\": logs_pd['latency_ms'].mean(),\n",
    "        \"p95_latency_ms\": logs_pd['latency_ms'].quantile(0.95),\n",
    "        \"prediction_mean\": logs_pd['prediction'].mean(),\n",
    "        \"drift_features_count\": sum(1 for r in drift_results.values() if r['drift_detected']),\n",
    "        \"total_alerts\": summary['total']\n",
    "    })\n",
    "\n",
    "print(\"\\n‚úì Reporte completo generado y registrado en MLflow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242af463",
   "metadata": {},
   "source": [
    "## üéâ Laboratorio Completado\n",
    "\n",
    "### Has aprendido a:\n",
    "\n",
    "‚úÖ **Desplegar modelos** con batch scoring y endpoints REST  \n",
    "‚úÖ **Configurar monitoreo** de latencia y m√©tricas de servicio  \n",
    "‚úÖ **Implementar logging** estructurado de predicciones  \n",
    "‚úÖ **Detectar data drift** usando pruebas estad√≠sticas  \n",
    "‚úÖ **Configurar alertas** automatizadas para incidentes  \n",
    "‚úÖ **Aplicar buenas pr√°cticas** de MLOps (tagging, documentaci√≥n)  \n",
    "\n",
    "### Pr√≥ximos Pasos\n",
    "\n",
    "1. **Automatizar monitoreo**: Crear job programado que ejecute este notebook diariamente\n",
    "2. **Integrar alertas**: Conectar con Slack, Teams o email para notificaciones\n",
    "3. **Implementar retraining**: Automatizar reentrenamiento cuando se detecte drift\n",
    "4. **Escalar a producci√≥n**: Mover a entorno productivo con gobernanza completa\n",
    "\n",
    "### Recursos Adicionales\n",
    "\n",
    "- [Databricks Model Serving](https://docs.databricks.com/machine-learning/model-serving/index.html)\n",
    "- [MLflow Deployments](https://mlflow.org/docs/latest/deployment/index.html)\n",
    "- [Model Monitoring Best Practices](https://www.databricks.com/blog/2022/04/19/model-monitoring-best-practices.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3300d95e",
   "metadata": {},
   "source": [
    "## Limpieza (Opcional)\n",
    "\n",
    "Ejecuta las siguientes celdas para limpiar recursos creados durante el laboratorio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af237411",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPCIONAL: Eliminar endpoint para evitar costos\n",
    "# Descomentar para ejecutar\n",
    "\n",
    "# try:\n",
    "#     client.delete_endpoint(endpoint_name)\n",
    "#     print(f\"‚úì Endpoint {endpoint_name} eliminado\")\n",
    "# except Exception as e:\n",
    "#     print(f\"Error al eliminar endpoint: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b561a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPCIONAL: Limpiar tablas temporales\n",
    "# Descomentar para ejecutar\n",
    "\n",
    "# import shutil\n",
    "# try:\n",
    "#     shutil.rmtree('/dbfs/tmp/predictions')\n",
    "#     shutil.rmtree('/dbfs/tmp/ml-monitoring')\n",
    "#     print(\"‚úì Tablas temporales eliminadas\")\n",
    "# except Exception as e:\n",
    "#     print(f\"Error al limpiar: {e}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
