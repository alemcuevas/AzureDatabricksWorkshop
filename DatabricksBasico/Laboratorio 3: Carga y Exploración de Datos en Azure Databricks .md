# üß™ Laboratorio 3: Carga y Exploraci√≥n de Datos Energ√©ticos en Azure Databricks

## üéØ Objetivo  
Conectar Azure Databricks a Azure Data Lake Storage Gen2 usando la clave de acceso, cargar el dataset `energy-consumption-by-source.csv`, explorarlo y guardarlo en formato Delta Lake.

---

## üïí Duraci√≥n estimada  
45 minutos

---

## ‚úÖ Prerrequisitos  
- Workspace de Azure Databricks activo  
- Cl√∫ster en estado **Running**  
- Archivo `energy-consumption-by-source.csv` subido al contenedor `energia` del Storage Account `storageenergydemo`  
- Clave de acceso disponible desde el portal de Azure

---

## üìù Pasos

### 1. Establecer la clave de acceso en la configuraci√≥n de Spark

Reemplaza `<clave_de_acceso>` con el valor de `key1` obtenido en el portal de Azure:

    spark.conf.set(
        "fs.azure.account.key.storageenergydemo.dfs.core.windows.net",
        "<clave_de_acceso>"
    )

![image](https://github.com/user-attachments/assets/fb584a61-c520-4b78-b4e0-47d066f2295b)

---

### 2. Cargar el archivo CSV en un DataFrame

    df = spark.read.option("header", True).csv("abfss://energia@storageenergydemo.dfs.core.windows.net/energy-consumption-by-source.csv")
    display(df)

![image](https://github.com/user-attachments/assets/a892a043-1e17-4d5e-9374-9b8f090b459b)

---

### 3. Verificar el esquema

    df.printSchema()

Deber√≠as ver columnas como:
- `Country`
- `Code`
- `Year`
- `Coal Consumption - EJ`
- `Oil Consumption - EJ`
- `Gas Consumption - EJ`
- `Nuclear Consumption - EJ`
- `Hydro Consumption - EJ`
- `Renewables Consumption - EJ`
- `Other Renewables Consumption - EJ`
- `Primary energy consumption (EJ)`

![image](https://github.com/user-attachments/assets/cbe79cc8-0907-4165-be33-a09ced3815d9)

---

### 4. Filtrar y explorar los datos

Mostrar los pa√≠ses √∫nicos:

    df.select("Country").distinct().show(10)

Ver los datos de M√©xico:

    df.filter(df["Country"] == "Mexico").display()

![image](https://github.com/user-attachments/assets/eaa55d59-f8e6-4fb0-b231-bc8e31e3f774)

![image](https://github.com/user-attachments/assets/6f666c96-5b89-4a0d-875f-52556e47bb38)

---
## üíæ ¬øPor qu√© guardar los datos en formato Delta Lake?

Guardar los datos en formato **Delta Lake** te ofrece m√∫ltiples beneficios que no tienes con formatos como CSV o Parquet. A continuaci√≥n se explican sus ventajas principales:

---

### üîÅ 1. Soporte para transacciones ACID

Delta Lake permite realizar **transacciones at√≥micas** sobre los datos, lo que significa que:
- Si una operaci√≥n falla, los datos no quedan en estado inconsistente
- Puedes hacer `update`, `delete`, `merge` de forma segura

---

### ‚èÆÔ∏è 2. Time Travel (viaje en el tiempo)

Puedes **consultar versiones anteriores** de la tabla con una sola l√≠nea de c√≥digo, √∫til para:
- Auditar cambios en los datos
- Restaurar datos eliminados o modificados por error
- Comparar estados hist√≥ricos

---

### ‚ö° 3. Rendimiento optimizado

- Delta Lake organiza los archivos internamente en un formato columnar (basado en Parquet)
- Mejora la velocidad de lectura, filtrado y agregaciones
- Soporta operaciones de **auto-compaction** y **Z-Ordering** para performance

---

### üßº 4. Integridad y limpieza de datos

Puedes combinar Delta Lake con operaciones como:
- `MERGE INTO` para hacer upserts (actualizar si existe, insertar si no)
- `VACUUM` para eliminar archivos obsoletos
- `OPTIMIZE` para compactar peque√±os archivos y mejorar el rendimiento

---

### üß† 5. Integraci√≥n con SQL y DataFrames

Las tablas Delta pueden ser consultadas directamente usando SQL o Spark DataFrames, como si fueran tablas de base de datos, sin importar el volumen de datos.

---

## ‚úÖ En resumen

Usar Delta Lake te da:
- Seguridad al escribir y actualizar datos
- Versionado autom√°tico
- Rendimiento superior
- Facilidad para trabajar con Spark SQL y ML pipelines

üí° **Recomendaci√≥n:** siempre que vayas a realizar an√°lisis exploratorio, procesamiento o machine learning con Spark, convierte tus datos a Delta desde el inicio.

---

### 5. Guardar los datos en formato Delta

Escribe el DataFrame completo como tabla Delta:

    df.write.format("delta").mode("overwrite").save("abfss://energia@storageenergydemo.dfs.core.windows.net/delta/energy-data")

![image](https://github.com/user-attachments/assets/6d7574d9-3f46-46b3-ac45-26df97ed6f85)

---

### 6. Leer los datos desde Delta Lake

    df_delta = spark.read.format("delta").load("abfss://energia@storageenergydemo.dfs.core.windows.net/delta/energy-data")
    display(df_delta)

![image](https://github.com/user-attachments/assets/ee432262-772b-4d05-a113-814155fc3a30)

---

### 7. Consultar los datos con SQL

1. Registrar el DataFrame como vista temporal:

        df_delta.createOrReplaceTempView("energia")

2. Consulta: pa√≠ses con mayor consumo de energ√≠a en 2020

        %sql
        SELECT Country, Year, `Primary energy consumption (EJ)`
        FROM energia
        WHERE Year = 2020
        ORDER BY `Primary energy consumption (EJ)` DESC
        LIMIT 10

![image](https://github.com/user-attachments/assets/98415037-06a8-4f9d-ab16-a319926289a1)

---

## üß† Conceptos clave aplicados

- Conexi√≥n directa a ADLS Gen2 v√≠a `spark.conf.set`  
- Lectura y visualizaci√≥n de archivos CSV con columnas reales del dataset  
- Escritura optimizada en formato Delta Lake  
- Consulta de datos energ√©ticos con Spark SQL

---

## üìö Recursos Oficiales Recomendados

- [Leer datos desde Azure Data Lake Gen2](https://learn.microsoft.com/azure/databricks/data/data-sources/azure/azure-datalake-gen2)  
- [Documentaci√≥n oficial de Delta Lake](https://learn.microsoft.com/azure/databricks/delta/)  
- [Consultas SQL en Databricks](https://learn.microsoft.com/azure/databricks/sql/)  
- [Funciones de Spark SQL](https://spark.apache.org/docs/latest/api/sql/index.html)

üí° **Consejo:** Siempre inspecciona el esquema y los nombres de columnas al cargar datasets CSV. El encabezado original puede contener espacios, s√≠mbolos o unidades.
