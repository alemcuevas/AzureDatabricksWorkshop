# ðŸ§ª Laboratorio 4: Procesamiento de Datos con Spark en Azure Databricks

## ðŸŽ¯ Objetivo  
Aplicar transformaciones y agregaciones sobre el dataset energÃ©tico usando Spark DataFrames y consultas SQL, utilizando como fuente una tabla Delta creada previamente.

---

## ðŸ•’ DuraciÃ³n estimada  
45 minutos

---

## âœ… Prerrequisitos  
- Haber ejecutado correctamente el Laboratorio 3  
- ClÃºster en estado **Running**  
- Archivo Delta en:  
  `abfss://energia@storageenergydemo.dfs.core.windows.net/delta/energy-data`

---

## ðŸ“ Pasos

### 1. Leer los datos Delta

    df = spark.read.format("delta").load("abfss://energia@storageenergydemo.dfs.core.windows.net/delta/energy-data")
    display(df)

ðŸ“¸ **Screenshot sugerido:** Vista de los datos cargados desde Delta

---

### 2. Verificar el esquema y las columnas

    df.printSchema()
    df.columns

Revisa que las columnas incluyan:
- `Country`
- `Year`
- `Coal Consumption - EJ`
- `Oil Consumption - EJ`
- ...
- `Primary energy consumption (EJ)`

ðŸ“¸ **Screenshot sugerido:** Resultado de `printSchema`

---

### 3. Limpiar datos: eliminar valores nulos

Eliminar registros donde no haya valor para la energÃ­a primaria consumida:

    df_clean = df.na.drop(subset=["Primary energy consumption (EJ)"])
    df_clean.count()

ðŸ“¸ **Screenshot sugerido:** Conteo de registros despuÃ©s de limpieza

---

### 4. Agregar columna: consumo en TWh

Convertimos de exajulios (EJ) a teravatios-hora (TWh), usando la equivalencia:

> 1 EJ = 277.778 TWh

    from pyspark.sql.functions import col

    df_transformed = df_clean.withColumn(
        "Primary energy consumption (TWh)",
        col("Primary energy consumption (EJ)") * 277.778
    )

ðŸ“¸ **Screenshot sugerido:** Nuevas columnas visualizadas con `display()`

---

### 5. Agrupar por paÃ­s: consumo total

    df_transformed.groupBy("Country") \
        .sum("Primary energy consumption (TWh)") \
        .orderBy("sum(Primary energy consumption (TWh))", ascending=False) \
        .show(10)

ðŸ“¸ **Screenshot sugerido:** Tabla de paÃ­ses con mayor consumo total

---

### 6. Crear vista temporal para SQL

    df_transformed.createOrReplaceTempView("energia")

Consulta: consumo promedio por dÃ©cada para MÃ©xico

    %sql
    SELECT
        FLOOR(Year/10)*10 AS Decada,
        AVG(`Primary energy consumption (TWh)`) AS Promedio_TWh
    FROM energia
    WHERE Country = 'Mexico'
    GROUP BY Decada
    ORDER BY Decada

ðŸ“¸ **Screenshot sugerido:** Tabla mostrando consumo promedio por dÃ©cada

---

### 7. Guardar resultados transformados

    df_transformed.write.format("delta") \
        .mode("overwrite") \
        .save("abfss://energia@storageenergydemo.dfs.core.windows.net/delta/energy-data-limpio")

ðŸ“¸ **Screenshot sugerido:** Celda de escritura completada correctamente

---

## ðŸ§  Conceptos clave aplicados

- Limpieza de datos con Spark (`na.drop`)  
- CreaciÃ³n de columnas nuevas con `withColumn`  
- ConversiÃ³n de unidades  
- Agrupaciones con `groupBy` y `agg`  
- Consultas SQL sobre vistas temporales  
- Escritura de resultados como tabla Delta optimizada

---

## ðŸ“š Recursos Oficiales Recomendados

- [Spark DataFrames en Azure Databricks](https://learn.microsoft.com/azure/databricks/data/dataframes)  
- [Transformaciones con PySpark](https://spark.apache.org/docs/latest/api/python/reference/index.html)  
- [Consultas SQL en notebooks](https://learn.microsoft.com/azure/databricks/sql/)  
- [ConversiÃ³n de formatos y escritura en Delta](https://learn.microsoft.com/azure/databricks/delta/delta-batch#write-to-a-table)

ðŸ’¡ **Consejo:** Este tipo de procesamiento es ideal como paso intermedio antes de realizar anÃ¡lisis de tendencias o entrenar modelos de machine learning.

