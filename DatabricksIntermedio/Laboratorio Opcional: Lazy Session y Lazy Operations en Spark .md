# ğŸ§ª Laboratorio Opcional: Lazy Session y Lazy Operations en Spark

## ğŸ¯ Objetivo

Explorar el comportamiento de ejecuciÃ³n diferida (lazy evaluation) y cÃ³mo Spark gestiona la ejecuciÃ³n de transformaciones y acciones dentro de una sesiÃ³n activa (lazy session).

---

## ğŸ•’ DuraciÃ³n estimada

15 â€“ 20 minutos

---

## âœ… Prerrequisitos

- ClÃºster en ejecuciÃ³n en Azure Databricks
- Familiaridad bÃ¡sica con transformaciones y acciones en Spark DataFrames

---

## ğŸ”¹ Â¿QuÃ© es una Lazy Session?

Una **lazy session** en Spark es una sesiÃ³n activa donde las operaciones declaradas (transformaciones) no se ejecutan de inmediato. Spark **espera hasta que se invoque una acciÃ³n** (como `.count()` o `.write()`) para disparar la ejecuciÃ³n real del plan lÃ³gico acumulado.

---

## ğŸ“ Pasos

### Paso 1: Crear un DataFrame sin ejecutar nada

df = spark.range(1, 100000000)

Esto solo define el DataFrame. No hay ejecuciÃ³n aÃºn. Puedes verificar en el Spark UI que no hay tareas activas.

---

### Paso 2: Aplicar transformaciones encadenadas

df2 = df.withColumn("triple", df["id"] * 3).filter("triple % 7 == 0")

TodavÃ­a no se ejecuta nada. Spark sigue agregando al plan lÃ³gico de forma perezosa (lazy).

---

### Paso 3: Ejecutar una acciÃ³n

df2.count()

AquÃ­ es cuando Spark ejecuta realmente todo el pipeline: crea el rango, calcula la columna `triple`, aplica el filtro y cuenta los resultados.

Observa cÃ³mo ahora aparece actividad en el Spark UI.

---

### Paso 4: Ejecutar otra acciÃ³n

df2.limit(5).display()

Aunque Spark ya ejecutÃ³ `.count()`, como el DataFrame es inmutable, volverÃ¡ a ejecutar toda la lÃ³gica desde el inicio (a menos que se haya hecho `.cache()`).

---

## ğŸ” Observaciones

- Spark no ejecuta nada hasta que se llama a una **acciÃ³n**
- Las **transformaciones** son perezosas (lazy)
- Las **acciones** son el disparador real de ejecuciÃ³n
- Este comportamiento permite optimizaciones internas automÃ¡ticas

---

## ğŸ’¡ Consejo

Si vas a reutilizar el resultado de un DataFrame complejo, usa `.cache()` o `.persist()` despuÃ©s de la primera ejecuciÃ³n para evitar re-calcular todo.

---

## ğŸ“š Recursos recomendados

- [DocumentaciÃ³n de Spark sobre ejecuciÃ³n diferida](https://spark.apache.org/docs/latest/rdd-programming-guide.html#lazy-evaluation)
- [GuÃ­a de transformaciones y acciones en DataFrames](https://learn.microsoft.com/azure/databricks/dataframes/datasets)

---

