## Laboratorio (Opcional): Caching y Persistencia en Spark

### üåü Objetivo
Comprender c√≥mo funcionan las operaciones de caching y persistencia en Spark y c√≥mo se relacionan con la ejecuci√≥n perezosa (lazy evaluation).

### ‚è∞ Duraci√≥n estimada
10 ‚Äì 15 minutos

### üîπ Descripci√≥n
Spark recalcula el pipeline completo cada vez que se ejecuta una acci√≥n sobre un DataFrame no almacenado. Este laboratorio explora c√≥mo usar `.cache()` y `.persist()` para mejorar la eficiencia.

---

### üîπ Pasos

#### 1. Crear un DataFrame con transformaciones pesadas

df = spark.range(1, 10000000)  
df_transformed = df.withColumn("cuadrado", df["id"] * df["id"]).filter("cuadrado % 3 == 0")

#### 2. Ejecutar una acci√≥n sin cache

df_transformed.count()

Verifica el tiempo de ejecuci√≥n. Luego vuelve a ejecutar otra acci√≥n:

df_transformed.limit(5).display()

Spark vuelve a recalcular todo el pipeline.

#### 3. Aplicar cache y ejecutar nuevamente

df_transformed.cache()  
df_transformed.count()  
df_transformed.limit(5).display()

Esta vez, la segunda acci√≥n ser√° mucho m√°s r√°pida.

#### 4. Usar persistencia (en disco o memoria)

from pyspark import StorageLevel  
df_transformed.persist(StorageLevel.MEMORY_AND_DISK)

Esto es √∫til cuando los datos no caben completamente en memoria.

---

### üîç Observaciones

- `.cache()` es equivalente a `.persist(StorageLevel.MEMORY_AND_DISK)` por defecto  
- Usa `.unpersist()` para liberar memoria cuando ya no se necesita  
- Cache solo tiene efecto tras la primera acci√≥n  

---

### ‚úÖ Consejo
Aplica cache o persistencia √∫nicamente a resultados intermedios que ser√°n reutilizados varias veces en un flujo de trabajo.
