# ðŸ§ª Laboratorio 6: OrquestaciÃ³n de Pipelines con Databricks Workflows

## ðŸŽ¯ Objetivo  
Crear un Workflow (job) en Databricks para orquestar tareas secuenciales de procesamiento de datos. Integrar la ejecuciÃ³n del workflow con Azure Data Factory para automatizaciÃ³n de extremo a extremo.

---

## ðŸ•’ DuraciÃ³n estimada  
30 minutos

---

## âœ… Prerrequisitos  
- Haber ejecutado los laboratorios anteriores  
- Al menos dos notebooks creados en Databricks (ej. `Ingesta_AutoLoader`, `Transformacion_Energia`)  
- Permisos para crear Workflows en el workspace  
- Tener acceso al portal de Azure y permisos para editar Azure Data Factory

---

## ðŸ“ Pasos

### 1. Crear notebooks base

1. En Databricks, crea dos notebooks:
   - `Ingesta_AutoLoader`: contiene cÃ³digo del Lab 2
   - `Transformacion_Energia`: contiene cÃ³digo del Lab 3 (o posteriores)

ðŸ“¸ **Screenshot sugerido:** Notebooks listos en tu Workspace

---

### 2. Crear un Workflow en Databricks

1. Ve al menÃº lateral > **Workflows**  
2. Haz clic en **Create Job**  
3. Asigna el nombre: `Pipeline_Energia`  
4. Agrega una primera tarea:
   - Task name: `Ingesta`
   - Type: Notebook
   - Notebook: `Ingesta_AutoLoader`
   - Cluster: escoge un cluster existente o configura uno nuevo

5. Agrega una segunda tarea:
   - Task name: `Transformacion`
   - Notebook: `Transformacion_Energia`
   - Establece dependencia: **"Runs after Ingesta"**

ðŸ“¸ **Screenshot sugerido:** Diagrama del flujo con las dos tareas

---

### 3. Probar ejecuciÃ³n del Workflow

1. Haz clic en **Run now**  
2. Espera a que las tareas se ejecuten correctamente  
3. Observa el output y duraciÃ³n de cada paso

ðŸ“¸ **Screenshot sugerido:** Pantalla de ejecuciÃ³n exitosa con status verde

---

### 4. Integrar con Azure Data Factory (ADF)

1. Abre Azure Data Factory > tu pipeline  
2. Agrega una actividad de tipo **Databricks notebook/job**  
3. Configura lo siguiente:
   - **Databricks Linked Service** apuntando a tu workspace
   - Tipo: Job
   - Job name: `Pipeline_Energia`

4. Guarda y publica el pipeline en ADF  
5. Ejecuta el pipeline manualmente o con un trigger programado

ðŸ“¸ **Screenshot sugerido:** Actividad de Databricks dentro de ADF

---

## ðŸ§  Conceptos clave aplicados

- Encadenamiento de notebooks en Databricks Workflows  
- AutomatizaciÃ³n de flujos ETL y ML  
- IntegraciÃ³n con herramientas de orquestaciÃ³n externas como Azure Data Factory  
- Control de ejecuciÃ³n y monitoreo de tareas programadas

---

## ðŸ“š Recursos Oficiales Recomendados

- [Workflows en Azure Databricks](https://learn.microsoft.com/azure/databricks/workflows/)  
- [Configurar Jobs programados](https://learn.microsoft.com/azure/databricks/jobs/)  
- [Conectar Azure Data Factory con Databricks](https://learn.microsoft.com/azure/data-factory/connector-azure-databricks)  
- [Best practices para pipelines de producciÃ³n](https://learn.microsoft.com/azure/databricks/dev-tools/best-practices#orchestration)

ðŸ’¡ **Consejo:** Usa Workflows para manejar la lÃ³gica de dependencias entre tareas, y ADF para agendar y monitorear flujos orquestados desde un solo punto de control.
