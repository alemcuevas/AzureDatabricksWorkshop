## Laboratorio (Opcional): Spark Execution Plan y Catalyst Optimizer

### ğŸŒŸ Objetivo
Entender cÃ³mo Spark transforma operaciones de alto nivel en un plan lÃ³gico, luego en un plan fÃ­sico, y cÃ³mo el optimizador Catalyst mejora la ejecuciÃ³n de consultas.

### â° DuraciÃ³n estimada
10 â€“ 15 minutos

### ğŸ”¹ DescripciÃ³n
Este laboratorio explica cÃ³mo Spark analiza y optimiza consultas utilizando el optimizador Catalyst. VerÃ¡s cÃ³mo inspeccionar el plan de ejecuciÃ³n y cÃ³mo las transformaciones no se ejecutan de inmediato, sino que son convertidas en un plan optimizado justo antes de ser ejecutadas.

---

### ğŸ”¹ Pasos

#### 1. Crear un DataFrame simple con transformaciones encadenadas

df = spark.range(1, 1000000)  
df_transformed = df.withColumn("doble", df["id"] * 2).filter("doble % 5 == 0").withColumn("final", df["id"] + 10)

#### 2. Mostrar el plan lÃ³gico y fÃ­sico

df_transformed.explain(True)

Este mÃ©todo muestra:
- El **plan lÃ³gico**: operaciones tal como fueron escritas
- El **plan optimizado**: despuÃ©s de aplicar reglas de Catalyst
- El **plan fÃ­sico**: operaciones especÃ­ficas que Spark ejecutarÃ¡ en los workers

#### 3. Ejecutar una acciÃ³n

df_transformed.limit(5).display()

Observa que es aquÃ­ cuando Spark ejecuta el plan fÃ­sico generado por Catalyst.

---

### ğŸ“Œ Â¿QuÃ© optimiza Catalyst?

- Reordenamiento de expresiones
- EliminaciÃ³n de columnas no utilizadas
- SimplificaciÃ³n de filtros
- CombinaciÃ³n de proyecciones
- Empuje de predicados (predicate pushdown)

---

### ğŸ§  Â¿Por quÃ© importa?

Catalyst ayuda a Spark a:
- Reducir la cantidad de datos procesados
- Evitar cÃ¡lculos innecesarios
- Ejecutar las consultas de forma mÃ¡s eficiente

---

### ğŸ” Observaciones

- Puedes usar `.explain(True)` en cualquier DataFrame para ver los tres niveles del plan
- TambiÃ©n puedes acceder al **Spark UI** para ver los stages ejecutados fÃ­sicamente
- Las optimizaciones de Catalyst ocurren **antes** de que empiece la ejecuciÃ³n real

---

### âœ… Consejo

Cuando tengas transformaciones complejas, usa `.explain(True)` para entender quÃ© estÃ¡ haciendo Spark realmente. Esto puede ayudarte a detectar operaciones innecesarias o mal ubicadas en tu pipeline.

