# ğŸ§ª Laboratorio 4: Tuning de Performance en Spark Jobs

## ğŸ¯ Objetivo  
Aplicar tÃ©cnicas de optimizaciÃ³n en Spark: uso de cache, particionamiento eficiente, y ajuste dinÃ¡mico de planes de ejecuciÃ³n con Adaptive Query Execution (AQE).

---

## ğŸ•’ DuraciÃ³n estimada  
45 minutos

---

## âœ… Prerrequisitos  
- Laboratorio 3 completado  
- Datos disponibles en la capa Silver:  
  `abfss://silver@storageenergydemo.dfs.core.windows.net/energy`  
- ClÃºster en estado **Running**, preferentemente con Databricks Runtime 8.4 o superior

---

## ğŸ“ Pasos

### 1. Leer los datos de Silver

    df = spark.read.format("delta").load("abfss://silver@storageenergydemo.dfs.core.windows.net/energy")
    display(df)

---

### 2. Activar Adaptive Query Execution (AQE)

    spark.conf.set("spark.sql.adaptive.enabled", "true")

âœ… AQE permite que Spark ajuste automÃ¡ticamente particiones y uniones segÃºn estadÃ­sticas observadas en tiempo de ejecuciÃ³n.

---

### 3. Analizar sin optimizaciÃ³n

Realiza una agregaciÃ³n sin cachÃ© ni particionamiento:

    from pyspark.sql.functions import avg

    df.groupBy("Country").agg(avg("Primary_energy_consumption_TWh")).show(10)

ğŸ“¸ **Screenshot sugerido:** Plan de ejecuciÃ³n sin cache

---

### 4. Aplicar cache para reutilizaciÃ³n

    df.cache()
    df.count()  # Obligamos materializaciÃ³n del cache

Repite una consulta para observar mejora de tiempo:

    df.groupBy("Year").count().show()

ğŸ“¸ **Screenshot sugerido:** Comparativa de tiempo entre consulta antes y despuÃ©s del cache

---

### 5. Aplicar particionamiento para escritura optimizada

    df.write.format("delta") \
      .mode("overwrite") \
      .partitionBy("Year") \
      .save("abfss://silver@storageenergydemo.dfs.core.windows.net/energy_partitioned")

ğŸ“¸ **Screenshot sugerido:** Tiempo de carga reducido al filtrar por aÃ±o

---

### 6. Consultar con filtro sobre particiÃ³n

    df_part = spark.read.format("delta").load("abfss://silver@storageenergydemo.dfs.core.windows.net/energy_partitioned")
    df_part.filter("Year = 2020").display()

âœ… Spark ahora puede hacer **partition pruning** para leer solo archivos del aÃ±o 2020

---

### 7. Comparar planes de ejecuciÃ³n

Usa el mÃ©todo `.explain(True)` para visualizar el efecto de AQE:

    df.groupBy("Country").agg(avg("Primary_energy_consumption_TWh")).explain(True)

ğŸ“¸ **Screenshot sugerido:** Plan de ejecuciÃ³n con AQE activo

---

## ğŸ§  Conceptos clave aplicados

- Uso de `cache()` para acelerar consultas repetidas  
- Escritura con `partitionBy()` para habilitar filtros eficientes  
- ActivaciÃ³n de AQE para ajuste dinÃ¡mico de planes de ejecuciÃ³n  
- AnÃ¡lisis de rendimiento con planes de ejecuciÃ³n detallados

---

## ğŸ“š Recursos Oficiales Recomendados

- [OptimizaciÃ³n con Spark Cache](https://spark.apache.org/docs/latest/sql-performance-tuning.html#caching-data)  
- [Particiones en Delta Lake](https://learn.microsoft.com/azure/databricks/delta/optimizations/file-mgmt#data-skipping)  
- [Adaptive Query Execution (AQE)](https://spark.apache.org/docs/latest/sql-performance-tuning.html#adaptive-query-execution)  
- [Databricks Performance Tuning Guide](https://learn.microsoft.com/azure/databricks/delta/performance/)

ğŸ’¡ **Consejo:** Estas tÃ©cnicas deben aplicarse una vez que se ha estabilizado el esquema de tus tablas y entiendes el patrÃ³n de acceso tÃ­pico.
